# %% [markdown]
# # Aplicaci√≥n Inteligente para el Pron√≥stico de Presupuesto Empresarial con Machine Learning e Integraci√≥n en Power BI
# 
# 
# ## Resumen del Proyecto
# 
# Este proyecto desarrolla una aplicaci√≥n inteligente de pron√≥stico presupuestario para empresas, integrando t√©cnicas avanzadas de aprendizaje autom√°tico, an√°lisis exploratorio de datos y modelos predictivos de series temporales. A trav√©s de la limpieza, transformaci√≥n y modelado de dos grandes datasets financieros ‚Äîcostos e ingresos‚Äî, la soluci√≥n permite estimar con precisi√≥n el total cost y net profit, identificando adem√°s patrones, anomal√≠as y variables clave que impactan en el presupuesto.
# 
# Se incluye una API opcional y una visualizaci√≥n automatizada mediante Power BI, permitiendo que los resultados se integren f√°cilmente en los flujos de trabajo empresariales.
# 
# ## Objetivo del Proyecto
# Desarrollar un sistema inteligente y automatizado de pron√≥stico de presupuesto que utilice t√©cnicas de aprendizaje autom√°tico y an√°lisis de datos para predecir con precisi√≥n los ingresos, costos y utilidades de una empresa.
# El sistema busca:
# 
# - Identificar los principales impulsores financieros.
# 
# - Detectar patrones y anomal√≠as en los datos hist√≥ricos.
# 
# - Construir modelos predictivos explicables que faciliten la toma de decisiones.
# 
# - Integrar los resultados en herramientas de visualizaci√≥n como Power BI.
# 
# - Automatizar el proceso de an√°lisis y generaci√≥n de reportes para uso empresarial continuo.
# 
# ## Plan de Trabajo
# Comprensi√≥n y Preprocesamiento de Datos
# 
# 1. Limpieza, transformaci√≥n y fusi√≥n de los datasets df_costs y df_earnings.
# 
#     - An√°lisis Exploratorio de Datos (EDA)
# 
# 2. Visualizaci√≥n de patrones temporales, correlaciones, y distribuci√≥n de variables clave.
# 
#     - Ingenier√≠a de Caracter√≠sticas
# 
# 3. Creaci√≥n de nuevas variables relevantes, codificaci√≥n de categor√≠as y manejo de valores faltantes.
# 
#     - Desarrollo del Modelo de Pron√≥stico
# 
# 4. Entrenamiento y comparaci√≥n de modelos como Random Forest, XGBoost, LightGBM y Prophet para pron√≥stico de m√∫ltiples variables objetivo.
# 
#     - Evaluaci√≥n y Validaci√≥n
# 
# 5. Validaci√≥n cruzada, an√°lisis de errores y m√©tricas como MAE, RMSE y R¬≤ para evaluar el desempe√±o.
# 
#     - Construcci√≥n de una API
# 
# 6. Desarrollo de una API REST con Streamlit para integrar el modelo en aplicaciones externas.
# 
#     - Integraci√≥n con Power BI
# 
# 7. Exportaci√≥n de predicciones y variables clave para su visualizaci√≥n din√°mica en paneles interactivos.
# 
#     - Automatizaci√≥n y Generaci√≥n de Reportes
# 
# 8. Implementaci√≥n de procesos autom√°ticos de an√°lisis y generaci√≥n de reportes financieros resumidos.
# 

# %% [markdown]
# ## Cargar Librer√≠as

# %%
# Bibliotecas incorporadas (biblioteca est√°ndar)
import math
import os
import warnings
from datetime import timedelta

# Manipulaci√≥n y an√°lisis de datos
import numpy as np
import pandas as pd

# Visualizaci√≥n
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Aprendizaje autom√°tico y preprocesamiento (scikit-learn, etc.)
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.cluster import KMeans
from sklearn.compose import ColumnTransformer
from sklearn.decomposition import PCA
from sklearn.ensemble import (
    IsolationForest,
    RandomForestRegressor,
    StackingRegressor,
    VotingRegressor,
)
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_selection import VarianceThreshold, mutual_info_regression
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.manifold import TSNE
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import (
    KFold,
    RandomizedSearchCV,
    TimeSeriesSplit,
    cross_val_score,
    train_test_split
)
from sklearn.multioutput import MultiOutputRegressor
from sklearn.neighbors import LocalOutlierFactor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import (
    FunctionTransformer,
    LabelEncoder,
    PowerTransformer,
    RobustScaler,
    StandardScaler
)

# Codificaci√≥n
from category_encoders import TargetEncoder

# Series temporales
from pandas.tseries.holiday import USFederalHolidayCalendar
from prophet import Prophet
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tools.eval_measures import rmse

# Estad√≠sticas
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# Utilidades
import joblib
from joblib import dump, load
import shap
import streamlit as st

# Configuraci√≥n (opcional)
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

import lightgbm as lgb
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor

# %% [markdown]
# ## Cargar Datos

# %%
# Carga el archivo de datos en un DataFrame
DATA_DIR = os.path.join(os.getcwd(), 'data')  # Uses current working directory
df_costs = pd.read_csv(os.path.join(DATA_DIR, 'costs_dataset.csv'))
df_earnings = pd.read_csv(os.path.join(DATA_DIR, 'earnings_dataset.csv'))

# %% [markdown]
# ## Preprocesamiento de Datos

# %% [markdown]
# ### Evaluaci√≥n Inicial de Datos

# %% [markdown]
# ### DataFrame df_costs

# %%
# imprime la informaci√≥n general/resumida sobre el DataFrame
print(df_costs.head())
print(df_costs.shape)
df_costs.info()
df_costs.describe()

# %% [markdown]
# - El DataFrame consta de **50,000 filas** y **25 columnas**.  
# - Campos incluidos:  
#   - `service_id`  
#   - `service_date`  
#   - `service_type`  
#   - `service_subtype`  
#   - `customer_type`  
#   - `contract_type`  
#   - `service_region`  
#   - `service_duration_hours`  
#   - `equipment_used`  
#   - `waste_volume_tons`  
#   - `waste_density`  
#   - `hazardous_material`  
#   - `recyclable_percentage`  
#   - `contamination_level`  
#   - `labor_cost`  
#   - `equipment_cost`  
#   - `transportation_cost`  
#   - `disposal_fees`  
#   - `regulatory_fees`  
#   - `fuel_price`  
#   - `weather_conditions`  
#   - `staff_availability`  
#   - `economic_index`  
#   - `service_delay`  
#   - `total_cost`  
# 
# **Problemas identificados**:  
# - La columna `service_date` tiene el tipo de dato "object" y debe convertirse al formato DateTime.
# 

# %%
# Cambiar la columna al formato de fecha y hora
df_costs['service_date'] = pd.to_datetime(df_costs['service_date'])
print("df_costs['service_date'] =", df_costs['service_date'])

# %% [markdown]
# #### DataFrame `df_earnings`

# %%
# imprime la informaci√≥n general/resumida sobre el DataFrame
print(df_earnings.head())
print(df_earnings.shape)
df_earnings.info()
df_earnings.describe()

# %% [markdown]
# - El DataFrame consta de **50,000 filas** y **13 columnas**.  
# - Campos incluidos:  
#   - `service_id`  
#   - `service_date`  
#   - `revenue`  
#   - `profit_margin_percentage`  
#   - `net_profit`  
#   - `billing_status`  
#   - `payment_method`  
#   - `invoice_id`  
#   - `payment_date`  
#   - `late_payment_days`  
#   - `discount_applied`  
#   - `discount_amount`  
#   - `currency`  
# 
# **Problemas identificados**:  
# - Las columnas `service_date` y `payment_date` tienen el tipo de dato "object" y deben convertirse al formato DateTime.
# 
# **Nota**: Ambos DataFrames comparten las columnas `service_id` y `service_date`, que se utilizar√°n para fusionar m√°s adelante.
# 

# %%
# Cambiar la columna al formato de fecha y hora
df_earnings['service_date'] = pd.to_datetime(df_earnings['service_date'])
print("df_earnings['service_date'] =", df_earnings['service_date'])
df_earnings['payment_date'] = pd.to_datetime(df_earnings['payment_date'])
print("df_earnings['payment_date'] =", df_earnings['payment_date'])

# %% [markdown]
# ### Manejo de Valores Faltantes

# %%
# Mostrar solo columnas con valores faltantes
# Para df_costs
missing_costs = df_costs.isnull().sum()
missing_costs = missing_costs[missing_costs > 0]
print("Valores faltantes en df_costs:\n", missing_costs)

# Para df_earnings
missing_earnings = df_earnings.isnull().sum()
missing_earnings = missing_earnings[missing_earnings > 0]
print("Valores faltantes en df_earnings:\n", missing_earnings)

# Verificar % de valores faltantes
# Para df_costs
missing_percent_costs = (df_costs.isnull().sum() / len(df_costs)) * 100
print(missing_percent_costs[missing_percent_costs > 0])

# Para df_earnings
missing_percent_earnings = (df_earnings.isnull().sum() / len(df_earnings)) * 100
print(missing_percent_earnings[missing_percent_earnings > 0])

# Visualizar valores faltantes

# Visualizar valores faltantes
plt.figure(figsize=(10,6))
sns.heatmap(df_costs.isnull(), cbar=False, cmap='viridis')
plt.title('Valores faltantes en df_costs')
plt.show()

plt.figure(figsize=(10,6))
sns.heatmap(df_earnings.isnull(), cbar=False, cmap='viridis')
plt.title('Valores faltantes en df_earnings')
plt.show()

# %% [markdown]
# ### Detecci√≥n de Duplicados

# %%
# Para df_costs
print("N√∫mero de filas duplicadas en df_costs:", df_costs.duplicated().sum())

# Para df_earnings
print("N√∫mero de filas duplicadas en df_earnings:", df_earnings.duplicated().sum())

# Para df_costs
duplicates_costs = df_costs[df_costs.duplicated()]
print("\nFilas duplicadas en df_costs:")
print(duplicates_costs)

# Para df_earnings
duplicates_earnings = df_earnings[df_earnings.duplicated()]
print("\nFilas duplicadas en df_earnings:")
print(duplicates_earnings)

# %% [markdown]
# ### Identificaci√≥n de Columnas Categ√≥ricas

# %%
# Identificar columnas categ√≥ricas
categorical_cols_costs = df_costs.select_dtypes(include=['object', 'category']).columns.tolist()
categorical_cols_earnings = df_earnings.select_dtypes(include=['object', 'category']).columns.tolist()

print("Columnas categ√≥ricas en df_costs:", categorical_cols_costs)
print("Columnas categ√≥ricas en df_earnings:", categorical_cols_earnings)

# %%
# Crear el codificador (Label Encoding)
le = LabelEncoder()

# Codificar cada columna categ√≥rica en df_costs
for col in categorical_cols_costs:
    df_costs[col] = le.fit_transform(df_costs[col].astype(str))
    print(f"Columna '{col}' codificada en df_costs")

# Codificar cada columna categ√≥rica en df_earnings  
for col in categorical_cols_earnings:
    df_earnings[col] = le.fit_transform(df_earnings[col].astype(str))
    print(f"Columna '{col}' codificada en df_earnings")

print("\nProceso de codificaci√≥n completado para ambas tablas")

# %% [markdown]
# ### Escalado/Normalizaci√≥n

# %%
# Identificar columnas num√©ricas para normalizaci√≥n
numeric_cols_costs = df_costs.select_dtypes(include=['float64', 'int64']).columns.tolist()
numeric_cols_earnings = df_earnings.select_dtypes(include=['float64', 'int64']).columns.tolist()

print("Columnas num√©ricas en df_costs:", numeric_cols_costs)
print("Columnas num√©ricas en df_earnings:", numeric_cols_earnings)

from sklearn.preprocessing import MinMaxScaler

# Crear instancia del escalador
scaler = MinMaxScaler()

# Normalizar datos en df_costs
df_costs_scaled = df_costs.copy()
df_costs_scaled[numeric_cols_costs] = scaler.fit_transform(df_costs_scaled[numeric_cols_costs])
print("\nNormalizaci√≥n completada para df_costs")

# Normalizar datos en df_earnings
df_earnings_scaled = df_earnings.copy()
df_earnings_scaled[numeric_cols_earnings] = scaler.fit_transform(df_earnings_scaled[numeric_cols_earnings])
print("Normalizaci√≥n completada para df_earnings")

print("\nProceso de escalado finalizado para ambos datasets")

# %% [markdown]
# ### Detecci√≥n de Outliers

# %%
# --- Funci√≥n para Detecci√≥n de Outliers ---
def detect_outliers(df, df_name="DataFrame", contamination=0.01):
    print(f"\nüîç Analizando outliers en {df_name}...")

    # Seleccionar columnas num√©ricas
    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
    print(f"üìä Se encontraron {len(numeric_cols)} columnas num√©ricas.")

    # Isolation Forest
    iso_forest = IsolationForest(contamination=contamination, random_state=42)
    df['outlier_iso'] = iso_forest.fit_predict(df[numeric_cols])
    n_outliers_iso = (df['outlier_iso'] == -1).sum()
    print(f"üå≤ Isolation Forest detect√≥ {n_outliers_iso} outliers ({100 * n_outliers_iso / len(df):.2f}%).")

    # Local Outlier Factor
    lof = LocalOutlierFactor(n_neighbors=20, contamination=contamination)
    df['outlier_lof'] = lof.fit_predict(df[numeric_cols])
    n_outliers_lof = (df['outlier_lof'] == -1).sum()
    print(f"üìé Local Outlier Factor detect√≥ {n_outliers_lof} outliers ({100 * n_outliers_lof / len(df):.2f}%).")

    return df, numeric_cols

# --- Funci√≥n para Visualizaci√≥n de Outliers ---
def visualize_outliers(df, numeric_cols, outlier_column='outlier_iso', title_suffix=''):
    if len(numeric_cols) < 2:
        print(f"‚ö†Ô∏è No hay suficientes columnas num√©ricas para graficar en {title_suffix}")
        return
    
    x_col = numeric_cols[0]
    y_col = numeric_cols[1]

    # Boxplot
    plt.figure(figsize=(12, 6))
    sns.boxplot(data=df[numeric_cols])
    plt.title(f'Boxplot de Variables Num√©ricas {title_suffix}')
    plt.xticks(rotation=45)
    plt.show()

    # Scatterplot
    plt.figure(figsize=(10, 6))
    sns.scatterplot(
        data=df,
        x=x_col,
        y=y_col,
        hue=outlier_column,
        palette={1: 'blue', -1: 'red'}
    )
    plt.title(f'Gr√°fico de Dispersi√≥n con Outliers ({outlier_column}) {title_suffix}')
    plt.legend(title='Outlier')
    plt.show()

# --- Funci√≥n para Manejo de Outliers ---
def handle_outliers(df, method="remove", contamination=0.01):
    """
    Detecta y maneja outliers autom√°ticamente, ya sea elimin√°ndolos o ajust√°ndolos.

    Par√°metros:
    df (DataFrame): El dataframe a procesar
    method (str): M√©todo para manejar outliers ("remove" para eliminar o "cap" para ajustar)
    contamination (float): Proporci√≥n esperada de outliers en los datos (usado para Isolation Forest y LOF)

    Retorna:
    DataFrame: Dataframe procesado con outliers manejados
    """
    # Seleccionar columnas num√©ricas
    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
    
    # Detecci√≥n con Isolation Forest
    iso_forest = IsolationForest(contamination=contamination, random_state=42)
    df['outlier_iso'] = iso_forest.fit_predict(df[numeric_cols])
    
    # Detecci√≥n con Local Outlier Factor
    lof = LocalOutlierFactor(n_neighbors=20, contamination=contamination)
    df['outlier_lof'] = lof.fit_predict(df[numeric_cols])
    
    # Mostrar resultados
    n_outliers_iso = (df['outlier_iso'] == -1).sum()
    n_outliers_lof = (df['outlier_lof'] == -1).sum()
    print(f"\nOutliers detectados en {df.name}: Isolation Forest = {n_outliers_iso}, LOF = {n_outliers_lof}")
    
    # Manejar outliers seg√∫n el m√©todo especificado
    if method == "remove":
        # Eliminar filas donde alg√∫n modelo detect√≥ outlier
        df_clean = df[(df['outlier_iso'] == 1) & (df['outlier_lof'] == 1)]
        print(f"Se eliminaron {len(df) - len(df_clean)} filas con outliers.")
        return df_clean
    
    elif method == "cap":
        # Ajustar outliers usando los percentiles 5 y 95
        for col in numeric_cols:
            lower_cap = df[col].quantile(0.05)
            upper_cap = df[col].quantile(0.95)
            df[col] = df[col].clip(lower=lower_cap, upper=upper_cap)
        print("Outliers ajustados a los percentiles 5 y 95.")
        return df

    else:
        print("M√©todo inv√°lido. Usar 'remove' o 'cap'.")
        return df

# --- An√°lisis Principal para df_costs ---
df_costs.name = "df_costs"  # Asignar nombre para referencia
df_costs, numeric_cols_costs = detect_outliers(df_costs, df_name="df_costs")
visualize_outliers(df_costs, numeric_cols_costs, outlier_column='outlier_iso', title_suffix='(df_costs)')

# Manejar outliers para df_costs (elegir "remove" o "cap")
df_costs_clean = handle_outliers(df_costs, method="remove")  # o method="cap"

# --- An√°lisis Principal para df_earnings ---
df_earnings.name = "df_earnings"  # Asignar nombre para referencia
df_earnings, numeric_cols_earnings = detect_outliers(df_earnings, df_name="df_earnings")
visualize_outliers(df_earnings, numeric_cols_earnings, outlier_column='outlier_iso', title_suffix='(df_earnings)')

# Manejar outliers para df_earnings (elegir "remove" o "cap")
df_earnings_clean = handle_outliers(df_earnings, method="remove")  # o method="cap"

# %%
# Combinar los dataframes usando 'service_id' y 'service_date', conservando todas las columnas
merged_df = pd.merge(
    df_costs, 
    df_earnings, 
    on=['service_id', 'service_date'], 
    how='outer', 
    suffixes=('_costos', '_ingresos')  # Cambi√© los sufijos a espa√±ol
)

# Verificar el dataframe combinado
print(f"Dimensiones del DataFrame combinado: {merged_df.shape}")
print("\nPrimeras filas del dataframe combinado:")
print(merged_df.head())  # Mostrar las primeras filas

# Verificar si hay duplicados despu√©s de la combinaci√≥n
duplicados = merged_df[merged_df.duplicated()]
if not duplicados.empty:
    print(f"\n‚ö†Ô∏è Se encontraron {len(duplicados)} filas duplicadas despu√©s de la combinaci√≥n.")
else:
    print("\n‚úÖ No se encontraron duplicados en el dataframe combinado.")

# Mostrar resumen de valores faltantes despu√©s de la combinaci√≥n
print("\nResumen de valores faltantes por columna:")
print(merged_df.isnull().sum())

# %% [markdown]
# ## An√°lisis Exploratorio de Datos (EDA)

# %% [markdown]
# ### Resumen de Datos y Estad√≠sticas Descriptivas para Columnas Num√©ricas

# %%
# An√°lisis Exploratorio de Datos (EDA)

# 1. Estad√≠sticas descriptivas
print("üîç Estad√≠sticas Descriptivas:")
print(merged_df.describe())

# 2. Valores faltantes
print(f"\nüîç Valores faltantes por columna:\n{merged_df.isnull().sum()}")

# 3. Visualizaci√≥n de distribuciones num√©ricas
print("\nüìä Visualizando distribuciones de columnas num√©ricas...")

# Histogramas
columnas_numericas = merged_df.select_dtypes(include=['float64', 'int64']).columns
merged_df[columnas_numericas].hist(figsize=(14, 12), bins=20, edgecolor='black', grid=False)
plt.suptitle("Distribuci√≥n de Variables Num√©ricas", y=1.02)
plt.tight_layout()
plt.show()

# Boxplots para outliers
print("\nüìä Visualizando Boxplots para detecci√≥n de outliers...")

plt.figure(figsize=(12, 8))
sns.boxplot(data=merged_df[columnas_numericas], orient="h", palette="Set2")
plt.title("Distribuci√≥n de Variables Num√©ricas (Boxplots)")
plt.tight_layout()
plt.show()

# 4. An√°lisis de correlaci√≥n
print("\nüìä Mapa de Correlaci√≥n entre Variables...")

plt.figure(figsize=(16, 12))
corr_matrix = merged_df.corr()
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # M√°scara para mostrar solo la mitad inferior
sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, cbar_kws={"shrink": .8})
plt.title("Matriz de Correlaci√≥n", pad=20)
plt.tight_layout()
plt.show()

# 5. An√°lisis de sesgo (skewness)
print("\nüîç An√°lisis de Sesgo en Variables Num√©ricas:")
sesgo = merged_df[columnas_numericas].skew()
print(sesgo)

# Visualizaci√≥n del sesgo
print("\nüìä Visualizaci√≥n del Grado de Sesgo...")

plt.figure(figsize=(14, 6))
sesgo.plot(kind='bar', color='darkcyan')
plt.axhline(y=0.5, color='r', linestyle='--')
plt.axhline(y=-0.5, color='r', linestyle='--')
plt.title("Grado de Sesgo en Variables Num√©ricas", pad=20)
plt.ylabel("Coeficiente de Sesgo")
plt.xticks(rotation=45)
plt.grid(axis='y', alpha=0.3)
plt.show()

# 6. An√°lisis de variables categ√≥ricas
columnas_categoricas = merged_df.select_dtypes(include=['object', 'category']).columns
if len(columnas_categoricas) > 0:
    print(f"\nüîç Valores √∫nicos en columnas categ√≥ricas:")
    for col in columnas_categoricas:
        print(f"{col}: {merged_df[col].nunique()} valores √∫nicos")
    
    # Visualizaci√≥n de variables categ√≥ricas
    print("\nüìä Distribuci√≥n de Variables Categ√≥ricas...")
    
    for col in columnas_categoricas:
        plt.figure(figsize=(10, 6))
        if merged_df[col].nunique() > 10:
            # Para variables con muchos valores √∫nicos
            sns.countplot(y=col, data=merged_df, order=merged_df[col].value_counts().index[:15])
            plt.title(f"Top 15 Valores m√°s Comunes en {col}")
        else:
            sns.countplot(x=col, data=merged_df)
            plt.title(f"Distribuci√≥n de {col}")
        plt.tight_layout()
        plt.show()
else:
    print("\n‚ÑπÔ∏è No se encontraron columnas categ√≥ricas para analizar.")

# %% [markdown]
# **Hallazgos**:  
# 
# 1. **Estad√≠sticas Descriptivas**:  
#    - El conjunto de datos contiene una mezcla de caracter√≠sticas num√©ricas (ej. `service_duration_hours`, `total_cost`, `net_profit`) y categ√≥ricas (ej. `service_type`, `payment_method`, `billing_status`).  
#    - Las estad√≠sticas clave muestran un amplio rango de valores en muchas caracter√≠sticas, como `net_profit`, que var√≠a entre 116.83 y 7977.77.  
#    - Varias columnas num√©ricas, como `discount_amount`, tienen una alta desviaci√≥n est√°ndar, lo que indica posibles outliers o variabilidad en los datos.  
# 
# 2. **Valores Faltantes**:  
#    - El conjunto de datos est√° limpio, sin valores faltantes en ninguna columna.  
# 
# 3. **Sesgo (Skewness)**:  
#    - Algunas caracter√≠sticas, como `discount_amount`, tienen un alto sesgo positivo (1.67), lo que sugiere que muchas entradas tienen valores bajos y pocas tienen valores significativamente altos.  
#    - La mayor√≠a de las otras caracter√≠sticas muestran un sesgo m√≠nimo, lo que implica distribuciones cercanas a la normalidad.  
# 
# 4. **Distribuciones de Caracter√≠sticas Num√©ricas**:  
#    - Los histogramas revelan que varias caracter√≠sticas, como `total_cost`, `revenue` y `net_profit`, tienen una distribuci√≥n sesgada a la derecha.  
#    - Los boxplots sugieren la presencia de outliers en varias caracter√≠sticas, como `total_cost` y `net_profit`, que deben investigarse m√°s adelante.  
# 
# 5. **Correlaci√≥n**:  
#    - El mapa de calor de correlaci√≥n indica que caracter√≠sticas como `total_cost`, `labor_cost`, `equipment_cost` y `transportation_cost` est√°n altamente correlacionadas entre s√≠.  
# 
# 6. **Caracter√≠sticas Categ√≥ricas**:  
#    - Los gr√°ficos de conteo revelan la distribuci√≥n de caracter√≠sticas categ√≥ricas como `billing_status` y `payment_method`.  
# 

# %% [markdown]
# ### Manejo de Datos Faltantes en Columnas Num√©ricas: Estrategia de Imputaci√≥n

# %%
# Verificar sesgo antes de la transformaci√≥n
print("Sesgo antes de la transformaci√≥n:")
print(merged_df[['discount_amount', 'net_profit']].skew())

# Aplicar transformaci√≥n logar√≠tmica a variables con sesgo positivo
merged_df['discount_amount_log'] = np.log1p(merged_df['discount_amount'])  # log(1+x) para manejar valores cero
merged_df['net_profit_log'] = np.log1p(merged_df['net_profit'])  # log(1+x) para manejar valores cero

# Verificar sesgo despu√©s de la transformaci√≥n
print("\nSesgo despu√©s de la transformaci√≥n:")
print(merged_df[['discount_amount_log', 'net_profit_log']].skew())

# Visualizar las transformaciones
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# Distribuciones originales
ax[0].hist(merged_df['discount_amount'], bins=50, color='skyblue', edgecolor='black')
ax[0].set_title('Distribuci√≥n Original de Descuentos')

ax[1].hist(merged_df['discount_amount_log'], bins=50, color='skyblue', edgecolor='black')
ax[1].set_title('Distribuci√≥n de Descuentos con Transformaci√≥n Logar√≠tmica')

plt.tight_layout()
plt.show()

# %% [markdown]
# **Hallazgos**:  
# 
# 1. **Sesgo Antes de la Transformaci√≥n**:  
#    - `discount_amount`: Sesgo de 1.67 (sesgo positivo fuerte).  
#    - `net_profit`: Sesgo de 0.66 (sesgo positivo moderado).  
# 
# 2. **Sesgo Despu√©s de la Transformaci√≥n Logar√≠tmica**:  
#    - `discount_amount_log`: Sesgo reducido a 0.95.  
#    - `net_profit_log`: Sesgo negativo (-0.55), lo que indica una distribuci√≥n m√°s equilibrada.  
# 
# 3. **Visualizaciones**:  
#    - Los histogramas muestran que, despu√©s de la transformaci√≥n logar√≠tmica, las distribuciones se vuelven m√°s sim√©tricas.  
# 

# %% [markdown]
# ### Detecci√≥n y Manejo de Outliers Usando el M√©todo IQR

# %%
# Calcular el Rango Intercuart√≠lico (IQR) para 'net_profit' y 'late_payment_days'
Q1 = merged_df[['net_profit', 'late_payment_days']].quantile(0.25)
Q3 = merged_df[['net_profit', 'late_payment_days']].quantile(0.75)
IQR = Q3 - Q1

# Identificar outliers usando el criterio de 1.5*IQR
outliers = ((merged_df[['net_profit', 'late_payment_days']] < (Q1 - 1.5 * IQR)) | 
           (merged_df[['net_profit', 'late_payment_days']] > (Q3 + 1.5 * IQR)))

# Mostrar cantidad de outliers por columna
print("üîç Outliers detectados:")
print(f"- 'net_profit': {outliers['net_profit'].sum()} ({(outliers['net_profit'].sum()/len(merged_df)*100):.2f}%)")
print(f"- 'late_payment_days': {outliers['late_payment_days'].sum()} ({(outliers['late_payment_days'].sum()/len(merged_df)*100):.2f}%)")

# %%
# Ajustar los outliers usando el m√©todo IQR (l√≠mites del bigote)
print("üîß Ajustando outliers en 'net_profit' usando el m√©todo IQR...")

# L√≠mite inferior
lim_inf = Q1['net_profit'] - 1.5 * IQR['net_profit']
merged_df['net_profit'] = np.where(
    merged_df['net_profit'] < lim_inf,
    lim_inf,
    merged_df['net_profit']
)

# L√≠mite superior
lim_sup = Q3['net_profit'] + 1.5 * IQR['net_profit']
merged_df['net_profit'] = np.where(
    merged_df['net_profit'] > lim_sup,
    lim_sup,
    merged_df['net_profit']
)

# Mostrar estad√≠sticas despu√©s del ajuste
print("\nüìä Estad√≠sticas de 'net_profit' despu√©s de ajustar outliers:")
print(merged_df['net_profit'].describe())

print(f"\nüí° Se ajustaron los valores fuera del rango [{lim_inf:.2f}, {lim_sup:.2f}]")
print("   Los valores por debajo del l√≠mite inferior se establecieron a", f"{lim_inf:.2f}")
print("   Los valores por encima del l√≠mite superior se establecieron a", f"{lim_sup:.2f}")

# %% [markdown]
# **Hallazgos**:  
# 
# 1. **Detecci√≥n de Outliers**:  
#    - `net_profit`: 223 outliers detectados.  
#    - `late_payment_days`: Ning√∫n outlier detectado.  
# 
# 2. **Ajuste de Outliers**:  
#    - Despu√©s de aplicar el ajuste, la columna `net_profit` ya no tiene valores extremos. El valor m√°ximo se redujo a 6394.15.  
# 
# 3. **Resumen Post-Ajuste**:  
#    - La columna `net_profit` tiene una media de 2439.54 y una desviaci√≥n est√°ndar de 1373.94.  
# 

# %% [markdown]
# ### Visualizaci√≥n de Distribuciones: Histogramas para Columnas Num√©ricas

# %%
# Columnas num√©ricas a visualizar
columnas_numericas = ['discount_amount', 'net_profit', 'late_payment_days', 'revenue', 'service_duration_hours']

print("üìä Generando histogramas para las siguientes columnas num√©ricas:", columnas_numericas)

# Configuraci√≥n de la figura
plt.figure(figsize=(16, 12))

# Generar histogramas y estad√≠sticas
for i, col in enumerate(columnas_numericas, 1):
    # Mostrar estad√≠sticas descriptivas
    print(f"\nüìå An√°lisis de {col}:")
    print(merged_df[col].describe().round(2))  # Redondeo a 2 decimales
    
    # Crear subplot
    plt.subplot(2, 3, i)
    
    # Histograma con mejoras visuales
    sns.histplot(
        data=merged_df,
        x=col,
        kde=True,  # A√±ade l√≠nea de densidad
        bins=30,
        color='#1f77b4',  # Color azul est√°ndar
        edgecolor='white',
        alpha=0.8
    )
    
    # Mejorar formato del t√≠tulo
    plt.title(f'Distribuci√≥n de {col}\n', fontsize=12, pad=10)
    plt.xlabel(col, fontsize=10)
    plt.ylabel('Frecuencia', fontsize=10)
    plt.grid(axis='y', alpha=0.3)

# Ajustes finales
plt.tight_layout(pad=2.5)
plt.suptitle('An√°lisis de Distribuci√≥n de Variables Num√©ricas', y=1.02, fontsize=14)
plt.show()

# Nota adicional
print("\nüí° Los histogramas incluyen:")
print("   - L√≠neas de densidad (KDE) para visualizar la forma de la distribuci√≥n")
print("   - Estad√≠sticas descriptivas completas para cada variable")
print("   - Escalas optimizadas para mejor visualizaci√≥n")

# %% [markdown]
# **Hallazgos**:  
# 
# 1. **`discount_amount`**:  
#    - Distribuci√≥n muy sesgada a la izquierda, con muchos valores en 0.00 (sin descuento).  
#    - Media: 76.75, Desviaci√≥n est√°ndar: 139.73.  
# 
# 2. **`net_profit`**:  
#    - Distribuci√≥n sesgada a la derecha, con valores entre 116.83 y 6394.15.  
#    - Media: 2439.54, Desviaci√≥n est√°ndar: 1373.94.  
# 
# 3. **`late_payment_days`**:  
#    - Distribuci√≥n uniforme, con mayor√≠a de valores alrededor de 30 d√≠as.  
# 
# 4. **`revenue`**:  
#    - Distribuci√≥n sesgada a la derecha, con valores entre 1580.53 y 27957.17.  
# 
# 5. **`service_duration_hours`**:  
#    - Distribuci√≥n uniforme, con valores entre 0.50 y 8.00 horas.  
# 

# %% [markdown]
# ### An√°lisis de Distribuci√≥n de Datos: Boxplots para Columnas Num√©ricas

# %%


# Mostrar resumen de boxplots
print("üìä Boxplots para las siguientes columnas num√©ricas:", columnas_numericas)

# Configurar el grid de visualizaci√≥n
plt.figure(figsize=(15, 10))

# Generar boxplots y estad√≠sticas para cada columna
for i, col in enumerate(columnas_numericas, 1):
    print(f"\nüîç Resumen estad√≠stico para {col}:")
    print(merged_df[col].describe())  # Mostrar estad√≠sticas descriptivas
    
    # Crear subplot
    plt.subplot(2, 3, i)
    
    # Boxplot con mejoras visuales
    sns.boxplot(
        x=merged_df[col], 
        color='#2ecc71',  # Verde m√°s profesional
        width=0.4,
        linewidth=1.5
    )
    
    # Formatear t√≠tulo y ejes
    plt.title(f'Distribuci√≥n de {col}', pad=12)
    plt.xlabel('Valores', fontsize=10)
    
    # A√±adir grid para mejor referencia
    plt.grid(axis='x', alpha=0.3)

# Ajustar layout y mostrar
plt.tight_layout(pad=2)
plt.suptitle('An√°lisis de Dispersi√≥n con Boxplots', y=1.02, fontsize=14)
plt.show()

# Notas explicativas
print("\nüí° An√°lisis de los boxplots:")
print("   - La l√≠nea central muestra la mediana (percentil 50)")
print("   - La caja abarca desde Q1 (25%) hasta Q3 (75%)")
print("   - Los bigotes muestran el rango t√≠pico de los datos (1.5*IQR)")
print("   - Los puntos fuera de los bigotes son valores at√≠picos potenciales")

# %% [markdown]
# **Hallazgos**:  
# 
# 1. **`discount_amount`**:  
#    - Confirmaci√≥n del sesgo a la izquierda, con muchos valores cercanos a cero.  
# 
# 2. **`net_profit`**:  
#    - Presencia de outliers en valores altos.  
# 
# 3. **`late_payment_days`**:  
#    - Distribuci√≥n uniforme, sin outliers.  
# 
# 4. **`revenue`**:  
#    - Outliers en valores altos.  
# 
# 5. **`service_duration_hours`**:  
#    - Distribuci√≥n compacta, sin outliers.  
# 

# %% [markdown]
# ### Gr√°ficos de Densidad para Visualizar Distribuciones

# %%
# Configuraci√≥n de gr√°ficos de densidad
print("üìä Gr√°ficos de densidad para las siguientes columnas num√©ricas:", columnas_numericas)

# Preparar la figura
plt.figure(figsize=(16, 10))

# Generar gr√°ficos de densidad para cada columna
for i, col in enumerate(columnas_numericas, 1):
    # Mostrar estad√≠sticas descriptivas
    print(f"\nüìå An√°lisis de densidad para {col}:")
    print(merged_df[col].describe().round(2))  # Redondeo a 2 decimales
    
    # Configurar subplot
    plt.subplot(2, 3, i)
    
    # Gr√°fico de densidad mejorado
    sns.kdeplot(
        data=merged_df,
        x=col,
        fill=True,  # Equivalente moderno a shade=True
        color='#e74c3c',  # Rojo profesional
        alpha=0.7,  # Transparencia
        linewidth=2  # Grosor de l√≠nea
    )
    
    # Formatear el gr√°fico
    plt.title(f'Distribuci√≥n de densidad de {col}\n', fontsize=12, pad=12)
    plt.xlabel(col, fontsize=10)
    plt.ylabel('Densidad', fontsize=10)
    plt.grid(alpha=0.3)  # Grid suave

# Ajustes finales
plt.tight_layout(pad=2.5)
plt.suptitle('An√°lisis de Densidad de Variables Num√©ricas', y=1.02, fontsize=14)
plt.show()

# Notas t√©cnicas
print("\nüí° Interpretaci√≥n de los gr√°ficos:")
print("   - Las √°reas bajo la curva representan la probabilidad")
print("   - Picos altos indican concentraci√≥n de valores")
print("   - Las colas muestran la dispersi√≥n de los datos")
print("   - Ideal para comparar distribuciones de forma normalizada")

# %% [markdown]
# **Hallazgos**:  
# 
# 1. **`discount_amount`**:  
#    - Pico pronunciado en cero, con cola larga hacia valores altos.  
# 
# 2. **`net_profit`**:  
#    - Distribuci√≥n unimodal con sesgo moderado a la derecha.  
# 
# 3. **`late_payment_days`**:  
#    - Distribuci√≥n normal centrada en 30 d√≠as.  
# 
# 4. **`revenue`**:  
#    - Sesgo a la derecha, con cola larga hacia valores altos.  
# 
# 5. **`service_duration_hours`**:  
#    - Distribuci√≥n unimodal con pico alrededor de 4 horas.  
# 

# %% [markdown]
# ### Exploraci√≥n Visual Integral: Histogramas, Boxplots y Gr√°ficos de Densidad

# %%
# Configuraci√≥n de visualizaciones
plots_por_columna = 3  # histograma, boxplot, densidad
total_columnas = len(columnas_numericas)
total_plots = total_columnas * plots_por_columna

# Calcular tama√±o del grid
cols = 3  # Puedes ajustar este valor
rows = math.ceil(total_plots / cols)

plt.figure(figsize=(cols * 5, rows * 3))

plot_index = 1
for col in columnas_numericas:
    print(f"\nüìä Resumen estad√≠stico para {col}:")
    print(merged_df[col].describe().round(2))  # Redondeo a 2 decimales

    # Histograma
    plt.subplot(rows, cols, plot_index)
    sns.histplot(
        merged_df[col], 
        kde=False, 
        bins=30, 
        color='#1f77b4',  # Azul est√°ndar
        edgecolor='white',
        alpha=0.8
    )
    plt.title(f'Distribuci√≥n de {col}', pad=10)
    plt.xlabel(col, fontsize=10)
    plt.ylabel('Frecuencia', fontsize=10)
    plt.grid(axis='y', alpha=0.3)
    plot_index += 1

    # Boxplot
    plt.subplot(rows, cols, plot_index)
    sns.boxplot(
        x=merged_df[col], 
        color='#2ecc71',  # Verde profesional
        width=0.4,
        linewidth=1.5
    )
    plt.title(f'Dispersi√≥n de {col}', pad=10)
    plt.xlabel('Valores', fontsize=10)
    plt.grid(axis='x', alpha=0.3)
    plot_index += 1

    # Gr√°fico de Densidad
    plt.subplot(rows, cols, plot_index)
    sns.kdeplot(
        merged_df[col], 
        shade=True, 
        color='#e74c3c',  # Rojo coral mejorado
        alpha=0.7
    )
    plt.title(f'Densidad de {col}', pad=10)
    plt.xlabel(col, fontsize=10)
    plt.ylabel('Densidad', fontsize=10)
    plt.grid(alpha=0.3)
    plot_index += 1

# Ajustes finales
plt.tight_layout(pad=2)
plt.suptitle('An√°lisis Visual Completo de Variables Num√©ricas', y=1.02, fontsize=14)
plt.show()

# Notas explicativas
print("\nüí° An√°lisis visual completo realizado:")
print("   - Histograma: muestra distribuci√≥n de frecuencias")
print("   - Boxplot: visualiza dispersi√≥n y valores at√≠picos")
print("   - Densidad: curva suavizada de la distribuci√≥n")
print("   - Todos los gr√°ficos comparten la misma escala para mejor comparaci√≥n")

# %% [markdown]
# **Hallazgos**:  
# 
# 1. **`discount_amount`**:  
#    - Histograma: Sesgo fuerte a la izquierda.  
#    - Boxplot: Outliers en valores altos.  
#    - Gr√°fico de densidad: Pico en cero.  
# 
# 2. **`net_profit`**:  
#    - Histograma: Distribuci√≥n extendida con sesgo a la derecha.  
#    - Boxplot: Outliers en valores altos.  
#    - Gr√°fico de densidad: Sesgo moderado a la derecha.  
# 
# 3. **`late_payment_days`**:  
#    - Histograma: Distribuci√≥n sim√©trica.  
#    - Boxplot: Sin outliers.  
#    - Gr√°fico de densidad: Distribuci√≥n normal.  
# 
# 4. **`revenue`**:  
#    - Histograma: Sesgo a la derecha.  
#    - Boxplot: Outliers en valores altos.  
#    - Gr√°fico de densidad: Cola larga hacia valores altos.  
# 
# 5. **`service_duration_hours`**:  
#    - Histograma: Distribuci√≥n sim√©trica.  
#    - Boxplot: Sin outliers.  
#    - Gr√°fico de densidad: Pico en 4 horas.  
# 

# %% [markdown]
# ## Conclusi√≥n del An√°lisis Exploratorio de Datos (EDA)
# 
# El EDA proporcion√≥ informaci√≥n valiosa sobre la estructura y relaciones en los datos, sentando las bases para el modelado y pron√≥stico. Los hallazgos clave incluyen:  
# 
# 1. **An√°lisis de Correlaci√≥n**:  
#    - Identificaci√≥n de relaciones entre variables clave.  
# 
# 2. **Ingenier√≠a de Caracter√≠sticas**:  
#    - Extracci√≥n de caracter√≠sticas temporales (ej. a√±o, mes) para capturar estacionalidad.  
# 
# 3. **Manejo de Valores Faltantes**:  
#    - Imputaci√≥n de valores faltantes en columnas num√©ricas.  
# 
# 4. **An√°lisis Temporal**:  
#    - Detecci√≥n de tendencias y fluctuaciones en el tiempo.  
# 
# 5. **Manejo de Outliers**:  
#    - Aplicaci√≥n de t√©cnicas como Z-score y winsorizaci√≥n.  
# 
# 6. **Escalado y Normalizaci√≥n**:  
#    - Estandarizaci√≥n de columnas num√©ricas.  
# 
# 7. **An√°lisis de Variables Categ√≥ricas**:  
#    - Visualizaci√≥n de distribuciones para identificar desequilibrios.  
# 
# **Reflexi√≥n Final**:  
# El EDA fue crucial para identificar patrones y problemas en los datos, asegurando que est√©n listos para las etapas de modelado y pron√≥stico.  
# 

# %% [markdown]
# ## Ingenier√≠a de Caracter√≠sticas

# %% [markdown]
# ### Adici√≥n de M√©tricas Financieras

# %%
# üìä C√°lculo de m√©tricas financieras clave

# ‚û§ Margen de ganancia en porcentaje
merged_df['profit_margin'] = (merged_df['net_profit'] / merged_df['revenue']) * 100  # Margen de ganancia (%)

# ‚û§ Monto final de la factura tras aplicar descuentos
merged_df['final_invoice_amount'] = merged_df['revenue'] - merged_df['discount_amount']  # Monto final despu√©s del descuento

# ‚û§ Ganancia bruta: ingresos totales menos costos totales
merged_df['gross_profit'] = merged_df['revenue'] - merged_df['total_cost']  # Ganancia bruta

# ‚û§ Ganancia neta: se recalcula para asegurar consistencia
merged_df['net_profit'] = merged_df['revenue'] - merged_df['total_cost']  # Ganancia neta

# üìâ Desglose porcentual de los distintos tipos de costos
merged_df['equipment_cost_percent'] = (merged_df['equipment_cost'] / merged_df['total_cost']) * 100  # % costo de equipo
merged_df['transportation_cost_percent'] = (merged_df['transportation_cost'] / merged_df['total_cost']) * 100  # % transporte
merged_df['disposal_fees_percent'] = (merged_df['disposal_fees'] / merged_df['total_cost']) * 100  # % tarifas de disposici√≥n
merged_df['regulatory_fees_percent'] = (merged_df['regulatory_fees'] / merged_df['total_cost']) * 100  # % tarifas regulatorias

# üßæ Estimaci√≥n de impuestos con tasa fija
tax_rate = 0.15
merged_df['tax_amount'] = merged_df['revenue'] * tax_rate  # Monto estimado de impuestos (15%)

# üí∞ C√°lculo de flujo de caja seg√∫n fechas de pago y servicio
merged_df['late_payment_days'] = (merged_df['payment_date'] - merged_df['service_date']).dt.days  # D√≠as de retraso en el pago

# ‚û§ Clasificaci√≥n del estado de pago en funci√≥n del retraso
merged_df['payment_status'] = pd.cut(
    merged_df['late_payment_days'],
    bins=[-float('inf'), 0, 30, float('inf')],
    labels=['Paid', 'Late', 'Overdue']  # Pagado, con retraso, moroso
)

# üö© Etiqueta de anomal√≠a en costos si el valor excede el percentil 75
high_cost_threshold = merged_df['total_cost'].quantile(0.75)
merged_df['cost_anomaly'] = np.where(
    merged_df['total_cost'] > high_cost_threshold, 'High', 'Normal'  # Alto si es mayor al 75% del costo total
)


# %% [markdown]
# Se agregaron m√©tricas clave como:  
# - `profit_margin` (margen de beneficio).  
# - `final_invoice_amount` (monto final de factura).  
# - `gross_profit` (beneficio bruto).  
# - `cost_breakdown_percentages` (desglose de costos).  
# - `tax_amount` (monto de impuestos).  
# - `cash_flow_prediction` (predicci√≥n de flujo de efectivo).  
# - `anomaly_flags` (banderas de anomal√≠as).  
# 

# %% [markdown]
# ### Extracci√≥n de Caracter√≠sticas de Fecha/Hora

# %%
# Extraer caracter√≠sticas de fecha/hora de 'service_date'
print("üîç Extrayendo caracter√≠sticas temporales de las fechas de servicio...")

merged_df['service_year'] = merged_df['service_date'].dt.year
merged_df['service_month'] = merged_df['service_date'].dt.month
merged_df['service_day'] = merged_df['service_date'].dt.day
merged_df['service_weekday'] = merged_df['service_date'].dt.weekday  # Lunes=0, Domingo=6
merged_df['service_hour'] = merged_df['service_date'].dt.hour
merged_df['service_is_weekend'] = merged_df['service_weekday'].isin([5, 6])  # S√°bado=5, Domingo=6

# Extraer caracter√≠sticas de fecha/hora de 'payment_date'
print("üí≥ Extrayendo caracter√≠sticas temporales de las fechas de pago...")

merged_df['payment_year'] = merged_df['payment_date'].dt.year
merged_df['payment_month'] = merged_df['payment_date'].dt.month
merged_df['payment_day'] = merged_df['payment_date'].dt.day
merged_df['payment_weekday'] = merged_df['payment_date'].dt.weekday
merged_df['payment_hour'] = merged_df['payment_date'].dt.hour
merged_df['payment_is_weekend'] = merged_df['payment_weekday'].isin([5, 6])

# Mostrar las nuevas columnas
print("\n‚úÖ Caracter√≠sticas temporales a√±adidas:")
print(merged_df[['service_date', 'payment_date', 
                'service_year', 'service_month', 'service_day',
                'service_weekday', 'service_hour', 'service_is_weekend',
                'payment_year', 'payment_month', 'payment_day',
                'payment_weekday', 'payment_hour', 'payment_is_weekend']].head())

# --------------------------------------------------
# Visualizaciones temporales
print("\nüìä Generando visualizaciones de distribuci√≥n temporal...")

# 1. Distribuci√≥n por a√±o de servicio
plt.figure(figsize=(12, 6))
sns.countplot(x='service_year', data=merged_df, palette='Blues_d')
plt.title('Distribuci√≥n por A√±o de Servicio', pad=15)
plt.xlabel('A√±o')
plt.ylabel('Cantidad de Servicios')
plt.grid(axis='y', alpha=0.3)
plt.show()

# 2. Distribuci√≥n por mes de servicio
plt.figure(figsize=(12, 6))
sns.countplot(x='service_month', data=merged_df, palette='Greens_d')
plt.title('Distribuci√≥n por Mes de Servicio', pad=15)
plt.xlabel('Mes')
plt.ylabel('Cantidad de Servicios')
plt.xticks(range(12), ['Ene', 'Feb', 'Mar', 'Abr', 'May', 'Jun', 'Jul', 'Ago', 'Sep', 'Oct', 'Nov', 'Dic'])
plt.grid(axis='y', alpha=0.3)
plt.show()

# 3. Distribuci√≥n por hora de servicio
plt.figure(figsize=(14, 6))
sns.countplot(x='service_hour', data=merged_df, palette='Oranges_d')
plt.title('Distribuci√≥n por Hora del Servicio', pad=15)
plt.xlabel('Hora del d√≠a')
plt.ylabel('Cantidad de Servicios')
plt.grid(axis='y', alpha=0.3)
plt.show()

# 4. Distribuci√≥n por d√≠a de la semana (servicio)
plt.figure(figsize=(12, 6))
sns.countplot(x='service_weekday', data=merged_df, palette='Purples_d')
plt.title('Servicios por D√≠a de la Semana', pad=15)
plt.xlabel('D√≠a de la semana')
plt.ylabel('Cantidad de Servicios')
plt.xticks(range(7), ['Lun', 'Mar', 'Mi√©', 'Jue', 'Vie', 'S√°b', 'Dom'])
plt.grid(axis='y', alpha=0.3)
plt.show()

# 5. Distribuci√≥n por a√±o de pago
plt.figure(figsize=(12, 6))
sns.countplot(x='payment_year', data=merged_df, palette='Reds_d')
plt.title('Distribuci√≥n por A√±o de Pago', pad=15)
plt.xlabel('A√±o')
plt.ylabel('Cantidad de Pagos')
plt.grid(axis='y', alpha=0.3)
plt.show()

# 6. D√≠as de retraso por d√≠a de pago (opcional)
plt.figure(figsize=(12, 6))
sns.boxplot(x='payment_weekday', y='late_payment_days', data=merged_df, palette='Set2')
plt.title('D√≠as de Retraso por D√≠a de la Semana de Pago', pad=15)
plt.xlabel('D√≠a de la semana de pago')
plt.ylabel('D√≠as de retraso')
plt.xticks(range(7), ['Lun', 'Mar', 'Mi√©', 'Jue', 'Vie', 'S√°b', 'Dom'])
plt.grid(axis='y', alpha=0.3)
plt.show()

print("\nüìå Notas:")
print("- Todas las visualizaciones muestran distribuciones temporales clave")
print("- Los d√≠as de la semana van de 0 (Lunes) a 6 (Domingo)")
print("- Los fines de semana est√°n marcados como S√°bado (5) y Domingo (6)")
print("- Las paletas de colores diferencian claramente cada tipo de an√°lisis")

# %% [markdown]
# Se extrajeron caracter√≠sticas como:  
# - A√±o, mes, d√≠a, d√≠a de la semana y hora de `service_date` y `payment_date`.  
# - Indicador de fin de semana.  

# %% [markdown]
# ### T√©rminos de Interacci√≥n

# %%
# Creaci√≥n de t√©rminos de interacci√≥n
merged_df['hour_weekend_interaction'] = merged_df['service_hour'] * merged_df['service_is_weekend']
merged_df['month_region_interaction'] = merged_df['service_month'] * merged_df['service_region']
merged_df['weekday_hazardous_material_interaction'] = merged_df['service_weekday'] * merged_df['hazardous_material'].astype(int)

# Mostrar primeras filas para verificar los nuevos t√©rminos de interacci√≥n
print("T√©rminos de Interacci√≥n A√±adidos:\n", merged_df[['service_hour', 'service_is_weekend', 'hour_weekend_interaction', 
                                              'service_month', 'service_region', 'month_region_interaction', 
                                              'service_weekday', 'hazardous_material', 
                                              'weekday_hazardous_material_interaction']].head())

# Visualizar interacci√≥n entre hora y fin de semana
plt.figure(figsize=(10, 6))
sns.countplot(x='hour_weekend_interaction', data=merged_df)
plt.title('Interacci√≥n entre Hora de Servicio y Fin de Semana')
plt.xlabel('Hora * Fin de Semana')
plt.ylabel('Conteo')
plt.show()

# Visualizar interacci√≥n entre mes y regi√≥n
plt.figure(figsize=(10, 6))
sns.countplot(x='month_region_interaction', data=merged_df)
plt.title('Interacci√≥n entre Mes de Servicio y Regi√≥n')
plt.xlabel('Mes * Regi√≥n')
plt.ylabel('Conteo')
plt.show()

# Visualizar interacci√≥n entre d√≠a de semana y material peligroso
plt.figure(figsize=(10, 6))
sns.countplot(x='weekday_hazardous_material_interaction', data=merged_df)
plt.title('Interacci√≥n entre D√≠a de Semana y Material Peligroso')
plt.xlabel('D√≠a de Semana * Material Peligroso')
plt.ylabel('Conteo')
plt.show()

# T√©rmino de interacci√≥n adicional: Hora de servicio y regi√≥n (ejemplo)
merged_df['hour_region_interaction'] = merged_df['service_hour'] * merged_df['service_region']
plt.figure(figsize=(10, 6))
sns.countplot(x='hour_region_interaction', data=merged_df)
plt.title('Interacci√≥n entre Hora de Servicio y Regi√≥n')
plt.xlabel('Hora * Regi√≥n')
plt.ylabel('Conteo')
plt.show()

# %% [markdown]
# Se crearon t√©rminos de interacci√≥n para capturar relaciones complejas entre caracter√≠sticas, como:  
# - Interacci√≥n entre hora y fin de semana.  
# - Interacci√≥n entre mes y regi√≥n.  

# %% [markdown]
# ### Binning

# %%
# Creaci√≥n de categor√≠as para 'service_duration_hours'
bins_service_duration = [0, 1, 2, 5, 10, 20, 50]  # L√≠mites personalizados (ajustar seg√∫n los datos)
labels_service_duration = ['0-1h', '1-2h', '2-5h', '5-10h', '10-20h', '20-50h']
merged_df['service_duration_binned'] = pd.cut(merged_df['service_duration_hours'], bins=bins_service_duration, labels=labels_service_duration)

# Creaci√≥n de categor√≠as para 'waste_volume_tons'
bins_waste_volume = [0, 0.5, 1, 2, 5, 10]  # L√≠mites personalizados (ajustar seg√∫n los datos)
labels_waste_volume = ['0-0.5t', '0.5-1t', '1-2t', '2-5t', '5-10t']
merged_df['waste_volume_binned'] = pd.cut(merged_df['waste_volume_tons'], bins=bins_waste_volume, labels=labels_waste_volume)

# Creaci√≥n de categor√≠as para 'fuel_price'
bins_fuel_price = [0, 1, 2, 3, 4, 5]  # L√≠mites personalizados (ajustar seg√∫n los datos)
labels_fuel_price = ['0-1', '1-2', '2-3', '3-4', '4-5']
merged_df['fuel_price_binned'] = pd.cut(merged_df['fuel_price'], bins=bins_fuel_price, labels=labels_fuel_price)

# Mostrar primeras filas para verificar las nuevas categor√≠as
print("Variables Categorizadas A√±adidas:\n", merged_df[['service_duration_hours', 'service_duration_binned', 
                                             'waste_volume_tons', 'waste_volume_binned', 
                                             'fuel_price', 'fuel_price_binned']].head())

# Visualizar distribuci√≥n de 'service_duration_binned'
plt.figure(figsize=(10, 6))
sns.countplot(x='service_duration_binned', data=merged_df)
plt.title('Distribuci√≥n de Duraci√≥n de Servicio por Categor√≠as')
plt.xlabel('Duraci√≥n de Servicio (horas)')
plt.ylabel('Conteo')
plt.show()

# Visualizar distribuci√≥n de 'waste_volume_binned'
plt.figure(figsize=(10, 6))
sns.countplot(x='waste_volume_binned', data=merged_df)
plt.title('Distribuci√≥n de Volumen de Residuos por Categor√≠as')
plt.xlabel('Volumen de Residuos (toneladas)')
plt.ylabel('Conteo')
plt.show()

# Visualizar distribuci√≥n de 'fuel_price_binned'
plt.figure(figsize=(10, 6))
sns.countplot(x='fuel_price_binned', data=merged_df)
plt.title('Distribuci√≥n de Precio de Combustible por Categor√≠as')
plt.xlabel('Precio de Combustible')
plt.ylabel('Conteo')
plt.show()

# %% [markdown]
# Variables continuas como `service_duration_hours`, `waste_volume_tons` y `fuel_price` se agruparon en categor√≠as para simplificar el an√°lisis.  

# %% [markdown]
# ### Agregaciones

# %%
# 1. Agregaci√≥n: Calcular media, suma y conteo para revenue, net_profit y labor_cost por service_type
agg_service_type = merged_df.groupby('service_type')[['revenue', 'net_profit', 'labor_cost']].agg(['mean', 'sum', 'count'])
print("Datos Agregados por Tipo de Servicio:\n", agg_service_type)

# 2. Agregaci√≥n: Calcular valores medios por service_region y service_month
agg_region_month = merged_df.groupby(['service_region', 'service_month'])[['revenue', 'net_profit']].agg('mean')
print("\nDatos Agregados por Regi√≥n y Mes de Servicio:\n", agg_region_month)

# 3. Agregaci√≥n: Conteo de servicios por service_region
service_count_region = merged_df.groupby('service_region').size()
print("\nConteo de Servicios por Regi√≥n:\n", service_count_region)

# 4. Agregaci√≥n: Conteo de servicios por payment_status
payment_status_count = merged_df.groupby('payment_status').size()
print("\nConteo de Servicios por Estado de Pago:\n", payment_status_count)

# 5. Visualizaci√≥n: Conteo de Servicios por Regi√≥n
plt.figure(figsize=(10, 6))
sns.countplot(x='service_region', data=merged_df)
plt.title('Conteo de Servicios por Regi√≥n')
plt.xlabel('Regi√≥n de Servicio')
plt.ylabel('N√∫mero de Servicios')
plt.xticks(rotation=90)
plt.show()

# 6. Visualizaci√≥n: Ingreso Promedio por Tipo de Servicio
agg_service_type['revenue']['mean'].plot(kind='bar', figsize=(10, 6))
plt.title('Ingreso Promedio por Tipo de Servicio')
plt.xlabel('Tipo de Servicio')
plt.ylabel('Ingreso Promedio')
plt.xticks(rotation=0)
plt.show()

# 7. Visualizaci√≥n: Ganancia Neta Promedio por Regi√≥n y Mes (Mapa de Calor)
pivot_net_profit = agg_region_month['net_profit'].unstack()
plt.figure(figsize=(12, 8))
sns.heatmap(pivot_net_profit, annot=True, cmap="YlGnBu", fmt=".2f")
plt.title('Ganancia Neta Promedio por Regi√≥n y Mes de Servicio')
plt.xlabel('Mes de Servicio')
plt.ylabel('Regi√≥n de Servicio')
plt.show()

# 8. Visualizaci√≥n: Valores At√≠picos en Costo Total por Regi√≥n (Diagrama de Caja)
plt.figure(figsize=(10, 6))
sns.boxplot(x='service_region', y='total_cost', data=merged_df)
plt.title('Valores At√≠picos en Costo Total por Regi√≥n')
plt.xlabel('Regi√≥n de Servicio')
plt.ylabel('Costo Total')
plt.show()

# 9. Visualizaci√≥n: Mapa de Calor de Correlaci√≥n para Variables Continuas
correlation_matrix = merged_df[['labor_cost', 'transportation_cost', 'net_profit', 'revenue', 'disposal_fees']].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title('Mapa de Calor de Correlaci√≥n')
plt.show()

# 10. Visualizaci√≥n: Ingreso Promedio por Duraci√≥n de Servicio (Categorizado)
avg_revenue_duration = merged_df.groupby('service_duration_binned')['revenue'].mean()
avg_revenue_duration.plot(kind='bar', figsize=(10, 6))
plt.title('Ingreso Promedio por Duraci√≥n de Servicio (Categorizado)')
plt.xlabel('Duraci√≥n de Servicio (Categor√≠as)')
plt.ylabel('Ingreso Promedio')
plt.xticks(rotation=45)
plt.show()

# %% [markdown]
# Se resumieron datos financieros y operativos por tipo de servicio, regi√≥n y mes, utilizando m√©tricas como promedio, suma y conteo.  

# %% [markdown]
# ### Codificaci√≥n por Objetivo (Target Encoding)

# %%
# 1. Choose the target variable (e.g., net_profit)
target = 'net_profit'

# 2. Select categorical columns for target encoding (e.g., service_type, service_region)
categorical_cols = ['service_type', 'service_region', 'service_subtype', 'customer_type', 'contract_type']

# 3. Perform Target Encoding
for col in categorical_cols:
    # Calculate the mean of target (net_profit) for each category in the column
    target_mean = merged_df.groupby(col)[target].mean()
    
    # Map the calculated mean to the original DataFrame
    merged_df[f'{col}_encoded'] = merged_df[col].map(target_mean)

    # Print some statistics about the encoding
    print(f"\nEncoding {col}:")
    print(merged_df[[col, f'{col}_encoded']].head())
    print(f"Mean of {target} per {col}:")
    print(target_mean.head())

# 4. Visualizing the encoded features
# Plot the distribution of the encoded columns for better understanding
for col in categorical_cols:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=f'{col}_encoded', y=target, data=merged_df)
    plt.title(f'Impact of Target Encoding on {col}')
    plt.xlabel(f'{col}_encoded')
    plt.ylabel(target)
    plt.xticks(rotation=45)
    plt.show()

# 5. Visualize correlations between encoded features and the target variable
encoded_cols = [f'{col}_encoded' for col in categorical_cols]

# Calculate the correlation matrix
correlation_matrix = merged_df[encoded_cols + [target]].corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap of Encoded Features with Target Variable')
plt.show()


# %% [markdown]
# Se aplic√≥ codificaci√≥n por objetivo a variables categ√≥ricas como `service_type` y `service_region`, transform√°ndolas en valores num√©ricos basados en la media de `net_profit`.  

# %% [markdown]
# ### Reducci√≥n de Dimensionalidad

# %%
# Preprocesamiento
# Eliminar columnas que no se usar√°n en la reducci√≥n de dimensionalidad
drop_columns = ['service_id', 'service_date', 'payment_date', 'invoice_id', 'payment_status', 'cost_anomaly']
numerical_df = merged_df.drop(columns=drop_columns)

# Eliminar filas con valores NaN
numerical_df = numerical_df.dropna()

# Seleccionar solo columnas num√©ricas
numerical_features = numerical_df.select_dtypes(include=[np.float64, np.int32, np.int64])

# Estandarizar los datos (importante para PCA y t-SNE)
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numerical_features)

# PCA: An√°lisis de Componentes Principales
# Realizar PCA y ver cu√°nta varianza explica cada componente
pca = PCA(n_components=2)  # Reducir a 2 dimensiones para visualizaci√≥n
pca_result = pca.fit_transform(scaled_data)

# Ratio de varianza explicada
print("Proporci√≥n de varianza explicada por cada componente:")
print(pca.explained_variance_ratio_)

# Crear DataFrame con los resultados de PCA
pca_df = pd.DataFrame(data=pca_result, columns=['PCA1', 'PCA2'])

# Visualizar los componentes de PCA
plt.figure(figsize=(8, 6))
sns.scatterplot(x='PCA1', y='PCA2', data=pca_df, alpha=0.5, color='purple')
plt.title('PCA de las Caracter√≠sticas')
plt.xlabel('PCA1')
plt.ylabel('PCA2')
plt.show()

# t-SNE: Embedding de Vecinos Estoc√°sticos t-Distribuido (para visualizaci√≥n de datos de alta dimensionalidad)
tsne = TSNE(n_components=2, random_state=42)
tsne_result = tsne.fit_transform(scaled_data)

# Crear DataFrame con los resultados de t-SNE
tsne_df = pd.DataFrame(data=tsne_result, columns=['tSNE1', 'tSNE2'])

# Visualizar los resultados de t-SNE
plt.figure(figsize=(8, 6))
sns.scatterplot(x='tSNE1', y='tSNE2', data=tsne_df, alpha=0.5, color='blue')
plt.title('t-SNE de las Caracter√≠sticas')
plt.xlabel('tSNE1')
plt.ylabel('tSNE2')
plt.show()

# Resumen de la reducci√≥n de dimensionalidad
print("\nResumen de PCA:")
print(f"Dimensi√≥n original de los datos: {scaled_data.shape}")
print(f"Dimensi√≥n reducida (despu√©s de PCA): {pca_result.shape}")

print("\nResumen de t-SNE:")
print(f"Dimensi√≥n original de los datos: {scaled_data.shape}")
print(f"Dimensi√≥n reducida (despu√©s de t-SNE): {tsne_result.shape}")

# Matriz de correlaci√≥n para observar colinealidad antes de la reducci√≥n de dimensionalidad
correlation_matrix = numerical_features.corr()

# Visualizar la matriz de correlaci√≥n
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=False, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title('Matriz de Correlaci√≥n de Caracter√≠sticas Num√©ricas')
plt.show()

# %% [markdown]
# Se aplicaron t√©cnicas como PCA y t-SNE para reducir la complejidad del conjunto de datos, conservando las caracter√≠sticas m√°s importantes.  

# %% [markdown]
# ## Selecci√≥n de Caracter√≠sticas

# %%
# 1. Plot the distribution of the target variable 'net_profit'
plt.figure(figsize=(10, 6))
sns.histplot(merged_df['net_profit'], kde=True)
plt.title('Distribution of Net Profit')
plt.xlabel('Net Profit')
plt.ylabel('Frequency')
plt.show()

# 2. Encode categorical variables (e.g., 'payment_status')
# For categorical columns with only two unique values, you can use LabelEncoder
label_encoder = LabelEncoder()
merged_df['payment_status_encoded'] = label_encoder.fit_transform(merged_df['payment_status'])

# 3. Drop non-numeric columns for correlation matrix (those with non-numeric values like categories or strings)
numeric_df = merged_df.select_dtypes(include=['float64', 'int32', 'int64'])

# Compute the correlation matrix for the numeric columns
correlation_matrix = numeric_df.corr()

# Print correlation with the target variable 'net_profit'
print(correlation_matrix['net_profit'].sort_values(ascending=False))

# 4. Scatter plots to visualize relationships between target and features
plt.figure(figsize=(10, 6))
sns.scatterplot(x=merged_df['total_cost'], y=merged_df['net_profit'])
plt.title('Total Cost vs Net Profit')
plt.xlabel('Total Cost')
plt.ylabel('Net Profit')
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(x=merged_df['revenue'], y=merged_df['net_profit'])
plt.title('Revenue vs Net Profit')
plt.xlabel('Revenue')
plt.ylabel('Net Profit')
plt.show()


# %% [markdown]
# #### Eliminaci√≥n de Columnas con Baja Varianza

# %%
# Mostrar el n√∫mero de caracter√≠sticas antes del filtrado
print(f"N√∫mero de caracter√≠sticas antes del umbral de varianza: {merged_df.shape[1]}")

# Separar columnas num√©ricas y de fecha
numeric_columns = merged_df.select_dtypes(include=['float64', 'int32', 'int64'])
date_columns = merged_df.select_dtypes(include=['datetime64'])

# Aplicar VarianceThreshold solo a columnas num√©ricas
selector = VarianceThreshold(threshold=0.01)
reduced_numeric_array = selector.fit_transform(numeric_columns)
selected_numeric_cols = numeric_columns.columns[selector.get_support()]

# Combinar columnas num√©ricas seleccionadas con todas las columnas de fecha
final_columns = list(selected_numeric_cols) + list(date_columns.columns)

# Filtrar el DataFrame original con las columnas finales
merged_df_reduced = merged_df[final_columns]

# Mostrar confirmaci√≥n
print(f"N√∫mero de caracter√≠sticas despu√©s de la selecci√≥n: {len(final_columns)}")
print("Columnas incluidas:", final_columns)

# Visualizar varianza de las caracter√≠sticas num√©ricas originales
variances = numeric_columns.var()
plt.figure(figsize=(12, 6))
variances.plot(kind='bar')
plt.title('Varianza de Cada Caracter√≠stica')
plt.ylabel('Varianza')
plt.xlabel('Caracter√≠stica')
plt.tight_layout()
plt.show()

# %% [markdown]
# #### Eliminaci√≥n de Caracter√≠sticas Altamente Correlacionadas

# %%
# Seleccionar solo columnas num√©ricas para el c√°lculo de correlaci√≥n
numeric_columns = merged_df.select_dtypes(include=[np.number])

# Calcular la matriz de correlaci√≥n
correlation_matrix = numeric_columns.corr()

# Graficar el mapa de calor de la matriz de correlaci√≥n
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Mapa de Calor de Matriz de Correlaci√≥n')
plt.show()

# Establecer umbral para alta correlaci√≥n
threshold = 0.9
drop_columns = set()

# Identificar columnas con alta correlaci√≥n
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > threshold:
            colname = correlation_matrix.columns[i]
            drop_columns.add(colname)

# Eliminar las columnas del dataframe original
merged_df_reduced = merged_df.drop(columns=drop_columns)

# Mostrar las columnas eliminadas por alta correlaci√≥n
print(f"Columnas eliminadas por alta correlaci√≥n: {drop_columns}")
print(f"N√∫mero de caracter√≠sticas despu√©s de eliminar columnas altamente correlacionadas: {merged_df_reduced.shape[1]}")

# %% [markdown]
# - **Eliminaci√≥n de Columnas con Baja Varianza**: Se eliminaron caracter√≠sticas con varianza menor a 0.01, reduciendo el conjunto de 77 a 61 caracter√≠sticas.  
# - **Eliminaci√≥n de Caracter√≠sticas Altamente Correlacionadas**: Se eliminaron 8 caracter√≠sticas con correlaci√≥n mayor a 0.9.  
# 

# %% [markdown]
# ### Manejo de Variables Categ√≥ricas

# %%
# Eliminar la columna original 'payment_status' (ya codificada)
merged_df_reduced.drop(columns=['payment_status'], inplace=True, errors='ignore')

# Convertir columnas categ√≥ricas binadas a c√≥digos num√©ricos
merged_df_reduced['service_duration_binned'] = merged_df_reduced['service_duration_binned'].cat.codes
merged_df_reduced['waste_volume_binned'] = merged_df_reduced['waste_volume_binned'].cat.codes
merged_df_reduced['fuel_price_binned'] = merged_df_reduced['fuel_price_binned'].cat.codes

# Verificar el DataFrame actualizado
print("DataFrame despu√©s de conversi√≥n:")
print(merged_df_reduced.head())

# %% [markdown]
# - Se codificaron variables categ√≥ricas utilizando `LabelEncoder` y `cat.codes`.  

# %% [markdown]
# ### Eliminaci√≥n de Caracter√≠sticas con Alto VIF

# %%
# 1. Identificar columnas de fecha para preservarlas
datetime_columns = merged_df_reduced.select_dtypes(include=['datetime', 'datetime64[ns]']).columns

# 2. Seleccionar columnas num√©ricas (excluyendo targets y fechas)
target_columns = ['total_cost', 'net_profit']
numeric_df = merged_df_reduced.drop(columns=target_columns + list(datetime_columns)).select_dtypes(include=['float64', 'int32', 'int64'])

# 3. Calcular matriz de correlaci√≥n
correlation_matrix = numeric_df.corr()

# 4. Identificar pares con alta correlaci√≥n (>0.9)
high_correlation_pairs = []
for column in correlation_matrix.columns:
    high_corr = correlation_matrix[column][correlation_matrix[column] > 0.9].index.tolist()
    if column in high_corr:
        high_corr.remove(column)  # Excluir autocorrelaci√≥n
    if high_corr:
        high_correlation_pairs.append((column, high_corr))

print("Pares altamente correlacionados (correlaci√≥n > 0.9):")
print(high_correlation_pairs)

# 5. Calcular Factor de Inflaci√≥n de Varianza (VIF)
X = add_constant(numeric_df, has_constant='add')
vif_data = pd.DataFrame()
vif_data["Variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Obtener variables con VIF alto (>10)
high_vif_features = vif_data[vif_data["VIF"] > 10]["Variable"].tolist()

print("\nVariables con VIF alto (>10):")
print(high_vif_features)

# 6. Combinar caracter√≠sticas a eliminar
columns_to_drop = set()
for _, correlated_features in high_correlation_pairs:
    columns_to_drop.update(correlated_features)
columns_to_drop.update(high_vif_features)

# Filtrar solo columnas existentes que no sean targets o fechas
columns_to_drop = [col for col in columns_to_drop 
                  if col not in target_columns and col not in datetime_columns]

# Verificar existencia de columnas
existing_columns_to_drop = [col for col in columns_to_drop if col in merged_df_reduced.columns]
missing_columns = [col for col in columns_to_drop if col not in merged_df_reduced.columns]

# Eliminar columnas de forma segura
merged_df_reduced = merged_df_reduced.drop(columns=existing_columns_to_drop)

# Mostrar resultados
print("\nColumnas eliminadas:", existing_columns_to_drop)
if missing_columns:
    print("‚ö†Ô∏è Columnas no encontradas (no eliminadas):", missing_columns)
print("Dimensi√≥n final del DataFrame:", merged_df_reduced.shape)
print("Columnas de fecha preservadas:", list(datetime_columns))

# %% [markdown]
# Se eliminaron caracter√≠sticas con alto Factor de Inflaci√≥n de Varianza (VIF) para evitar multicolinealidad.  

# %% [markdown]
# ### Selecci√≥n Final de Caracter√≠sticas

# %%
# 1. Identificar columnas datetime y crear caracter√≠sticas derivadas manteniendo las originales
datetime_columns = merged_df_reduced.select_dtypes(include=['datetime', 'datetime64[ns]']).columns

# Guardar columnas datetime originales y nuevas caracter√≠sticas
original_date_columns = list(datetime_columns)  # Conservar originales
engineered_date_features = []

for column in datetime_columns:
    # Crear nuevas caracter√≠sticas manteniendo la columna original
    days_col = f'{column}_days_since_start'
    month_col = f'{column}_month'
    dow_col = f'{column}_dayofweek'
    weekend_col = f'{column}_is_weekend'
    timestamp_col = f'{column}_timestamp'

    # Crear caracter√≠sticas derivadas
    merged_df_reduced[days_col] = (merged_df_reduced[column] - merged_df_reduced[column].min()).dt.days
    merged_df_reduced[month_col] = merged_df_reduced[column].dt.month
    merged_df_reduced[dow_col] = merged_df_reduced[column].dt.dayofweek
    merged_df_reduced[weekend_col] = merged_df_reduced[column].dt.dayofweek.isin([5, 6]).astype(int)
    merged_df_reduced[timestamp_col] = merged_df_reduced[column].astype(np.int64)

    engineered_date_features.extend([days_col, month_col, dow_col, weekend_col, timestamp_col])

# 2. Identificar y codificar columnas categ√≥ricas
categorical_columns = merged_df_reduced.select_dtypes(include=['object']).columns

label_encoder = LabelEncoder()
for col in categorical_columns:
    merged_df_reduced[col] = label_encoder.fit_transform(merged_df_reduced[col])

# 3. Definir caracter√≠sticas y objetivos (manteniendo columnas datetime originales)
X = merged_df_reduced.drop(columns=['total_cost', 'net_profit'])  # Conservar columnas datetime
y = merged_df_reduced[['total_cost', 'net_profit']]

# 4. Divisi√≥n train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Identificar columnas no datetime
non_datetime_cols = [col for col in X.columns if col not in datetime_columns]

# Crear transformador para columnas datetime
datetime_transformer = FunctionTransformer(
    lambda x: x.astype(np.int64), 
    validate=False
)

preprocessor = ColumnTransformer(
    transformers=[
        ('datetime', datetime_transformer, datetime_columns),
        ('passthrough', 'passthrough', non_datetime_cols)
    ])

# Crear pipeline
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('model', MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42)))
])

print("Entrenando modelo para m√∫ltiples variables objetivo...")
pipeline.fit(X_train, y_train)
print("Entrenamiento del modelo completado.")

# 6. Obtener importancia de caracter√≠sticas (manejar salida de ColumnTransformer)
# Obtener nombres de caracter√≠sticas despu√©s de transformaci√≥n
feature_names = (list(datetime_columns) + non_datetime_cols)

# Obtener importancias de todos los estimadores
importances = []
for estimator in pipeline.named_steps['model'].estimators_:
    importances.append(estimator.feature_importances_)

# Crear DataFrame de importancias
importances_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance_total_cost': importances[0],
    'Importance_net_profit': importances[1]
})

# 7. Caracter√≠sticas m√°s importantes
print("\nTop 20 caracter√≠sticas importantes para predecir Total Cost:")
print(importances_df.sort_values(by='Importance_total_cost', ascending=False).head(20))

print("\nTop 20 caracter√≠sticas importantes para predecir Net Profit:")
print(importances_df.sort_values(by='Importance_net_profit', ascending=False).head(20))

# 8. Visualizaci√≥n
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
sns.barplot(data=importances_df.sort_values(by='Importance_total_cost', ascending=False).head(20),
            x='Importance_total_cost', y='Feature', palette='viridis')
plt.title('Top 20 caracter√≠sticas para predecir Total Cost')

plt.subplot(1, 2, 2)
sns.barplot(data=importances_df.sort_values(by='Importance_net_profit', ascending=False).head(20),
            x='Importance_net_profit', y='Feature', palette='viridis')
plt.title('Top 20 caracter√≠sticas para predecir Net Profit')

plt.tight_layout()
plt.show()

# 9. Seleccionar caracter√≠sticas top + incluir datetime
top_features_total_cost = importances_df.sort_values(by='Importance_total_cost', ascending=False).head(30)['Feature'].tolist()
top_features_net_profit = importances_df.sort_values(by='Importance_net_profit', ascending=False).head(30)['Feature'].tolist()

# Combinar e incluir caracter√≠sticas importantes m√°s datetime originales
top_features_combined = list(set(top_features_total_cost + top_features_net_profit + original_date_columns))
print("\nCaracter√≠sticas top combinadas para ambos objetivos (incluyendo datetime originales):")
print(top_features_combined)

# 10. Crear conjunto reducido de caracter√≠sticas
X_selected = X[top_features_combined]
print(f"\nN√∫mero de caracter√≠sticas seleccionadas: {X_selected.shape[1]}")

# 11. Mostrar todo en X_selected
print("\n‚úÖ Caracter√≠sticas finales en X_selected (incluyendo datetime originales):")
print(X_selected.columns.tolist())

print("\nüìä Vista previa de X_selected:")
display(X_selected.head())

# %% [markdown]
# Se identificaron las 10 caracter√≠sticas m√°s importantes para `total_cost` y `net_profit` utilizando `RandomForestRegressor`. Las caracter√≠sticas clave incluyen:  
# - `profit_margin_percentage`.  
# - `transportation_cost_percent`.  
# - `cost_anomaly`.  
# 

# %% [markdown]
# ## Conclusi√≥n de Importancia y Selecci√≥n de Caracter√≠sticas
# 
# El an√°lisis de importancia de caracter√≠sticas fue crucial para identificar los predictores m√°s influyentes en el pron√≥stico de **costo total** y **beneficio neto**. Utilizando un `RandomForestRegressor` en un framework `MultiOutputRegressor`, se evalu√≥ la contribuci√≥n relativa de cada caracter√≠stica. A continuaci√≥n, los hallazgos clave:
# 
# ---
# 
# ### 1. **Top 10 Caracter√≠sticas para Costo Total**  
#    - **Dominadas por m√©tricas financieras**:  
#      - `cost_anomaly` (importancia: 0.552) ‚Üí Principal predictor, indicando que los costos an√≥malos son clave.  
#      - `transportation_cost_percent`, `labor_cost`, `regulatory_fees_percent`.  
#    - **Otras cr√≠ticas**:  
#      - `transportation_cost` y `regulatory_fees` para pron√≥sticos precisos.  
# 
# ---
# 
# ### 2. **Top 10 Caracter√≠sticas para Beneficio Neto**  
#    - **Predictores principales**:  
#      - `profit_margin_percentage` (importancia: 0.639) ‚Üí Impacto directo en el beneficio neto.  
#      - `cost_anomaly` y `transportation_cost_percent` tambi√©n significativos.  
#    - **Patr√≥n similar**:  
#      - La detecci√≥n de costos an√≥malos es relevante para ambos objetivos.  
# 
# ---
# 
# ### 3. **Conjunto Combinado de Caracter√≠sticas**  
#    - **Selecci√≥n final**: 32 caracter√≠sticas, incluyendo:  
#      - Las m√°s importantes para ambos targets.  
#      - Variables temporales (`service_month`, `payment_day`).  
#    - **Ventaja**:  
#      - Combina patrones temporales y m√©tricas financieras.  
# 
# ---
# 
# ### 4. **Selecci√≥n Final y Beneficios**  
#    - **Modelo optimizado**:  
#      - M√°s interpretable y con menor riesgo de sobreajuste.  
#    - **Impacto**:  
#      - Precisi√≥n mejorada en pron√≥sticos y insights accionables.  
# 
# ---
# 
# **Resumen**:  
# Este proceso permiti√≥ identificar predictores clave, optimizar el modelo y garantizar que capture tanto relaciones financieras como temporales. El resultado es un sistema robusto para la toma de decisiones basada en datos.  

# %% [markdown]
# ## Preparaci√≥n para el Modelado

# %% [markdown]
# ### Pipeline de Entrenamiento y Evaluaci√≥n de Modelos

# %%
# Columnas objetivo
target_columns = ['total_cost', 'net_profit']

# Definir caracter√≠sticas y objetivos - trabajar con copia
X = X_selected.copy()
y_total_cost = merged_df_reduced['total_cost'].copy()
y_net_profit = merged_df_reduced['net_profit'].copy()

# Transformador personalizado para caracter√≠sticas datetime
class DateTimeFeatureAdder(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.datetime_cols = None
        
    def fit(self, X, y=None):
        # Identificar columnas datetime
        self.datetime_cols = X.select_dtypes(include=['datetime', 'datetime64[ns]']).columns.tolist()
        return self
        
    def transform(self, X):
        X_transformed = X.copy()
        for col in self.datetime_cols:
            # Crear caracter√≠sticas num√©ricas desde datetime
            X_transformed[f'{col}_timestamp'] = X[col].astype(np.int64) // 10**9  # Convertir a segundos
            X_transformed[f'{col}_year'] = X[col].dt.year
            X_transformed[f'{col}_month'] = X[col].dt.month
            X_transformed[f'{col}_day'] = X[col].dt.day
            X_transformed[f'{col}_dayofweek'] = X[col].dt.dayofweek
            X_transformed[f'{col}_dayofyear'] = X[col].dt.dayofyear
        return X_transformed

# Transformador personalizado para eliminar columnas datetime originales
class DateTimeColumnDropper(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.datetime_cols = None
        
    def fit(self, X, y=None):
        return self
        
    def transform(self, X):
        # Eliminar columnas datetime originales para evitar errores en LightGBM
        datetime_cols = X.select_dtypes(include=['datetime', 'datetime64[ns]']).columns.tolist()
        return X.drop(columns=datetime_cols)

# Crear pipeline de preprocesamiento
preprocessor = Pipeline([
    ('add_datetime_features', DateTimeFeatureAdder()),
    ('drop_original_datetime', DateTimeColumnDropper()),
    ('passthrough_numeric', FunctionTransformer(lambda x: x))  # Pasar otras columnas
])

# Divisi√≥n de datos para cada objetivo
def train_test_split_target(X, y):
    return train_test_split(X, y, test_size=0.2, shuffle=False)

# M√©tricas de evaluaci√≥n
def evaluate_model(y_true, y_pred, label):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    print(f"\nüìä M√©tricas de Evaluaci√≥n ({label}):")
    print(f"  - MAE:  {mae:.2f}")
    print(f"  - RMSE: {rmse:.2f}")
    print(f"  - R¬≤:   {r2:.3f}")

    plt.figure(figsize=(8, 5))
    plt.scatter(y_true, y_pred, alpha=0.3)
    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], '--r')
    plt.xlabel("Valor Real")
    plt.ylabel("Predicci√≥n")
    plt.title(f"Real vs. Predicci√≥n - {label}")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    return mae, rmse, r2

# Entrenar modelo y evaluar
def train_and_evaluate(X, y, label):
    X_train, X_test, y_train, y_test = train_test_split_target(X, y)
    
    model = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', lgb.LGBMRegressor(
            n_estimators=100,
            random_state=42,
            verbose=-1
        ))
    ])
    
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    return evaluate_model(y_test, preds, label)

# Validaci√≥n cruzada temporal
def time_series_cv(X, y, label):
    print(f"\nüîÅ Validaci√≥n Cruzada Temporal ({label})")
    tscv = TimeSeriesSplit(n_splits=5)
    maes, rmses, r2s = [], [], []

    for i, (train_idx, test_idx) in enumerate(tscv.split(X)):
        X_tr, X_te = X.iloc[train_idx], X.iloc[test_idx]
        y_tr, y_te = y.iloc[train_idx], y.iloc[test_idx]

        model = Pipeline([
            ('preprocessor', preprocessor),
            ('regressor', lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1))
        ])
        
        model.fit(X_tr, y_tr)
        preds = model.predict(X_te)

        mae, rmse, r2 = evaluate_model(y_te, preds, f"Fold {i+1} - {label}")
        maes.append(mae)
        rmses.append(rmse)
        r2s.append(r2)

    print(f"\nüìà Resultados Promedio CV ({label}):")
    print(f"  - MAE Promedio:  {np.mean(maes):.2f}")
    print(f"  - RMSE Promedio: {np.mean(rmses):.2f}")
    print(f"  - R¬≤ Promedio:   {np.mean(r2s):.3f}")

# Ejecutar pipeline
print("\nüöÄ Iniciando Entrenamiento y Evaluaci√≥n de Modelos...")

# Verificar presencia de service_date
print("\nüîç Columnas originales en X:")
print(X.columns.tolist())

# Verificar columnas datetime
datetime_cols = X.select_dtypes(include=['datetime', 'datetime64[ns]']).columns.tolist()
print("\nüîç Columnas datetime identificadas:")
print(datetime_cols)

# Ejecutar modelos
print("\n=== Modelo Total Cost ===")
train_and_evaluate(X, y_total_cost, 'Total Cost')
time_series_cv(X, y_total_cost, 'Total Cost')

print("\n=== Modelo Net Profit ===")
train_and_evaluate(X, y_net_profit, 'Net Profit')
time_series_cv(X, y_net_profit, 'Net Profit')

# Verificaci√≥n final de service_date en datos originales
print("\n‚úÖ Verificaci√≥n - service_date en datos originales:")
print("service_date" in merged_df_reduced.columns)

# %% [markdown]
# **Rendimiento General del Modelo**:  
# Ambas variables objetivo ‚Äî **Costo Total** y **Beneficio Neto** ‚Äî fueron evaluadas usando LightGBM con un conjunto de hold-out y validaci√≥n cruzada TimeSeriesSplit.  
# Los modelos demostraron un rendimiento excepcionalmente fuerte en ambos objetivos.
# 
# **Costo Total**
# 
# **Evaluaci√≥n Hold-out**
# - **MAE:** 225.13
# - **RMSE:** 307.60
# - **R¬≤:** 0.992
# 
# **Validaci√≥n Cruzada TimeSeries (5 folds)**
# 
# | Fold | MAE    | RMSE   | R¬≤    |
# |------|--------|--------|-------|
# | 1    | 261.90 | 360.66 | 0.989 |
# | 2    | 229.23 | 314.03 | 0.991 |
# | 3    | 226.52 | 311.09 | 0.992 |
# | 4    | 219.05 | 303.62 | 0.992 |
# | 5    | 219.01 | 298.42 | 0.992 |
# 
# **Resultados Promedio CV:**
# - **MAE Promedio:** 231.14
# - **RMSE Promedio:** 317.56
# - **R¬≤ Promedio:** 0.991
# 
# **Interpretaci√≥n**
# El modelo predice el Costo Total con alta precisi√≥n y consistencia en el tiempo.  
# - Puntajes R¬≤ cercanos a 0.991‚Äì0.992 muestran que el modelo captura casi toda la varianza.
# - Los bajos valores de MAE/RMSE indican errores de predicci√≥n peque√±os incluso en divisiones temporales de datos.
# 
# **Beneficio Neto**
# 
# **Evaluaci√≥n Hold-out**
# - **MAE:** 86.66
# - **RMSE:** 126.80
# - **R¬≤:** 0.992
# 
# **Validaci√≥n Cruzada TimeSeries (5 folds)**
# 
# | Fold | MAE    | RMSE   | R¬≤    |
# |------|--------|--------|-------|
# | 1    | 98.53  | 141.24 | 0.990 |
# | 2    | 89.96  | 130.59 | 0.991 |
# | 3    | 87.05  | 125.96 | 0.992 |
# | 4    | 86.56  | 126.13 | 0.992 |
# | 5    | 87.38  | 128.52 | 0.991 |
# 
# **Resultados Promedio CV:**
# - **MAE Promedio:** 89.90
# - **RMSE Promedio:** 130.49
# - **R¬≤ Promedio:** 0.991
# 
# **Interpretaci√≥n**
# Las predicciones para Beneficio Neto son incluso m√°s ajustadas que para Costo Total.  
# - Los valores R¬≤ nuevamente se acercan a 0.991‚Äì0.992, indicando un excelente ajuste del modelo.
# - Los bajos MAE/RMSE reflejan pron√≥sticos precisos con errores promedio peque√±os.
# 
# **Conclusiones Clave**
# - Ambos modelos son robustos y se generalizan bien en divisiones temporales.
# - R¬≤ cercano a 1 en todas las evaluaciones confirma fuertes relaciones caracter√≠sticas-objetivo.
# - M√©tricas de error bajas y estables (MAE, RMSE) hacen que estos modelos sean confiables para pron√≥sticos financieros.
# - No se observ√≥ sobreajuste significativo ‚Äî los resultados de hold-out y CV est√°n estrechamente alineados.

# %% [markdown]
# ### Modelo Base: SARIMA (Univariado)

# %%
# 1. Preparaci√≥n de datos - Crear copia de trabajo preservando el dataframe original
df_for_modeling = merged_df_reduced.copy()

# Asegurar formato datetime para 'service_date'
df_for_modeling['service_date'] = pd.to_datetime(df_for_modeling['service_date'], errors='coerce')

# Eliminar filas con fechas inv√°lidas
df_for_modeling = df_for_modeling.dropna(subset=['service_date'])

# Asegurar que los targets sean num√©ricos
df_for_modeling['total_cost'] = pd.to_numeric(df_for_modeling['total_cost'], errors='coerce')
df_for_modeling['net_profit'] = pd.to_numeric(df_for_modeling['net_profit'], errors='coerce')

# Eliminar filas con valores faltantes en los targets
df_for_modeling = df_for_modeling.dropna(subset=['total_cost', 'net_profit'])

# Establecer √≠ndice datetime y ordenar (sin eliminar la columna service_date)
df_for_modeling = df_for_modeling.set_index('service_date', drop=False).sort_index()

# Verificar que service_date se conserva como √≠ndice y columna
print("\nColumnas en el dataframe de trabajo:", df_for_modeling.columns.tolist())
print("√çndice es service_date:", df_for_modeling.index.name)

# 2. Funci√≥n mejorada de modelo SARIMA con m√©tricas adicionales
def sarima_model(data, target_col, title):
    print(f"\nEntrenando modelo SARIMA para {title}...")
    
    # Extraer la serie objetivo (usando el √≠ndice que es service_date)
    y = data[target_col]
    
    # Verificar que la serie sea num√©rica
    if not pd.api.types.is_numeric_dtype(y):
        raise ValueError(f"La columna objetivo '{target_col}' debe ser num√©rica")
    
    # Dividir datos (80% entrenamiento, 20% prueba)
    train_size = int(len(y) * 0.8)
    train, test = y.iloc[:train_size], y.iloc[train_size:]
    
    # Ajustar modelo SARIMA
    try:
        model = SARIMAX(train,
                       order=(1, 1, 1),
                       seasonal_order=(1, 1, 1, 12),
                       enforce_stationarity=False,
                       enforce_invertibility=False)
        model_fit = model.fit(disp=False)
        
        # Pron√≥stico
        predictions = model_fit.get_forecast(steps=len(test))
        pred_ci = predictions.conf_int()
        pred_mean = predictions.predicted_mean
        
        # Graficar resultados
        plt.figure(figsize=(12, 6))
        plt.plot(train.index, train, label='Entrenamiento')
        plt.plot(test.index, test, label='Real')
        plt.plot(test.index, pred_mean, label='Pron√≥stico')
        plt.fill_between(test.index,
                        pred_ci.iloc[:, 0],
                        pred_ci.iloc[:, 1], color='k', alpha=0.1)
        plt.title(f'Pron√≥stico SARIMA para {title}')
        plt.legend()
        plt.grid(True)
        plt.show()
        
        # Calcular m√©tricas
        mae_value = mean_absolute_error(test, pred_mean)
        rmse_value = rmse(test, pred_mean)
        r2_value = r2_score(test, pred_mean)
        
        # Mostrar todas las m√©tricas
        print(f"\nM√©tricas de Rendimiento para {title}:")
        print(f"  - MAE:  {mae_value:.2f}")
        print(f"  - RMSE: {rmse_value:.2f}")
        print(f"  - R¬≤:   {r2_value:.3f}")
        
        return {
            'model': model_fit,
            'metrics': {
                'MAE': mae_value,
                'RMSE': rmse_value,
                'R2': r2_value
            }
        }
        
    except Exception as e:
        print(f"Error ajustando modelo SARIMA para {title}: {str(e)}")
        return None

# 3. Ejecutar modelos y recolectar m√©tricas
print("Iniciando modelado SARIMA...")

# Diccionario para almacenar todas las m√©tricas
all_metrics = {}

# Modelo para total_cost
total_cost_result = sarima_model(df_for_modeling, 'total_cost', 'Costo Total')
if total_cost_result:
    all_metrics['Costo Total'] = total_cost_result['metrics']

# Modelo para net_profit
net_profit_result = sarima_model(df_for_modeling, 'net_profit', 'Beneficio Neto')
if net_profit_result:
    all_metrics['Beneficio Neto'] = net_profit_result['metrics']

# Calcular y mostrar m√©tricas promedio si ambos modelos se ejecutaron correctamente
if all_metrics:
    mean_mae = np.mean([metrics['MAE'] for metrics in all_metrics.values()])
    mean_rmse = np.mean([metrics['RMSE'] for metrics in all_metrics.values()])
    mean_r2 = np.mean([metrics['R2'] for metrics in all_metrics.values()])
    
    print("\n" + "="*50)
    print("Rendimiento Promedio en Todos los Modelos:")
    print(f"  - MAE Promedio:  {mean_mae:.2f}")
    print(f"  - RMSE Promedio: {mean_rmse:.2f}")
    print(f"  - R¬≤ Promedio:   {mean_r2:.3f}")
    print("="*50)

# Verificar que el dataframe original permanece sin cambios
print("\nVerificaci√≥n:")
print("Columnas del dataframe original:", merged_df_reduced.columns.tolist())
print("Forma del dataframe original:", merged_df_reduced.shape)
print("Columnas del dataframe de trabajo:", df_for_modeling.columns.tolist())
print("service_date en dataframe de trabajo:", 'service_date' in df_for_modeling.columns)

# %% [markdown]
# **Resultados de Rendimiento**
# 
# | M√©trica       | Costo Total | Beneficio Neto | Promedio |
# |--------------|-------------|----------------|----------|
# | **MAE**      | 2800.27     | 1133.11        | 1966.69  |
# | **RMSE**     | 3392.80     | 1375.10        | 2383.95  |
# | **R¬≤**       | -0.001      | -0.001         | -0.001   |
# 
# **Hallazgos Clave**
# 
# 1. **Rendimiento Predictivo Deficiente**:
#    - Valores de R¬≤ ‚âà -0.001 indican que el modelo no supera una l√≠nea base simple (media)
#    - Errores absolutos altos (MAE > 1000 para ambos targets)
# 
# 2. **Limitaciones Identificadas**:
#    - Enfoque univariado ignora predictores clave (tipo de servicio, precios, etc.)
#    - Posible no estacionariedad no capturada en los par√°metros (1,1,1)(1,1,1,12)
#    - Datos podr√≠an contener relaciones no lineales o ruido no modelado
# 
# 3. **Diagn√≥stico Adicional**:
#    - Aunque el modelo convergi√≥ t√©cnicamente, su utilidad predictiva es limitada
#    - Los residuos muestran patrones no modelados
# 
# **Conclusi√≥n**
# El modelo SARIMA base **no es adecuado** para este caso de uso. Se recomienda:
# - Explorar modelos multivariados (ARIMAX)
# - Considerar enfoques de machine learning que capturen relaciones complejas
# - Realizar pruebas de estacionariedad m√°s exhaustivas si se insiste con SARIMA

# %% [markdown]
# ### Pipeline del Modelo Prophet para Pron√≥sticos

# %%
# Crear copia de trabajo del dataframe
df_for_prophet = merged_df_reduced.copy()

# Funci√≥n para entrenar y evaluar Prophet
def prophet_forecast(data, target_col, title):
    print(f"\nEjecutando Prophet para {title}...")
    
    # Preparar dataframe para Prophet
    prophet_df = data[['service_date', target_col]].dropna().copy()
    prophet_df = prophet_df.rename(columns={'service_date': 'ds', target_col: 'y'})
    
    # Divisi√≥n train/test (80/20)
    train_size = int(len(prophet_df) * 0.8)
    train_df = prophet_df.iloc[:train_size]
    test_df = prophet_df.iloc[train_size:]
    
    # Instanciar y entrenar Prophet
    model = Prophet()
    model.fit(train_df)

    # Crear dataframe futuro para per√≠odo de prueba
    future = test_df[['ds']].copy()
    
    # Pron√≥stico
    forecast = model.predict(future)
    y_pred = forecast['yhat'].values
    y_true = test_df['y'].values
    
    # Evaluaci√≥n
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    
    print(f"M√©tricas de evaluaci√≥n para {title}:")
    print(f"  - MAE:  {mae:.2f}")
    print(f"  - RMSE: {rmse:.2f}")
    print(f"  - R¬≤:   {r2:.3f}")
    
    # Gr√°fico
    plt.figure(figsize=(12, 6))
    plt.plot(train_df['ds'], train_df['y'], label='Entrenamiento')
    plt.plot(test_df['ds'], test_df['y'], label='Prueba')
    plt.plot(test_df['ds'], y_pred, label='Pron√≥stico')
    plt.fill_between(test_df['ds'],
                     forecast['yhat_lower'], forecast['yhat_upper'],
                     color='gray', alpha=0.2)
    plt.title(f"Pron√≥stico Prophet para {title}")
    plt.legend()
    plt.grid(True)
    plt.show()
    
    return {
        'model': model,
        'metrics': {'MAE': mae, 'RMSE': rmse, 'R2': r2}
    }

# Ejecutar Prophet para ambos targets
prophet_results = {}

# Prophet para total_cost
res_total = prophet_forecast(df_for_prophet, 'total_cost', 'Costo Total')
prophet_results['Costo Total'] = res_total['metrics']

# Prophet para net_profit
res_profit = prophet_forecast(df_for_prophet, 'net_profit', 'Beneficio Neto')
prophet_results['Beneficio Neto'] = res_profit['metrics']

# M√©tricas promedio
mean_mae = np.mean([m['MAE'] for m in prophet_results.values()])
mean_rmse = np.mean([m['RMSE'] for m in prophet_results.values()])
mean_r2 = np.mean([m['R2'] for m in prophet_results.values()])

print("\n" + "="*50)
print("Rendimiento Promedio (Prophet):")
print(f"  - MAE Promedio:  {mean_mae:.2f}")
print(f"  - RMSE Promedio: {mean_rmse:.2f}")
print(f"  - R¬≤ Promedio:   {mean_r2:.3f}")
print("="*50)

# %% [markdown]
# **Resultados de Rendimiento**
# 
# | M√©trica     | Costo Total | Beneficio Neto | Promedio |
# |------------|-------------|----------------|----------|
# | **MAE**    | 2829.02     | 1132.27        | 1980.65  |
# | **RMSE**   | 3417.16     | 1389.01        | 2403.09  |
# | **R¬≤**     | -0.002      | -0.001         | -0.001   |
# 
# **Hallazgos Clave**
# 
# 1. **Rendimiento Sub√≥ptimo**:
#    - Valores de R¬≤ negativos indican peor desempe√±o que un modelo b√°sico de media
#    - Errores absolutos altos (MAE > 1000) en ambas variables objetivo
# 
# 2. **Posibles Causas**:
#    - Naturaleza compleja/no estacional de los datos financieros
#    - Limitaci√≥n univariada (no considera otras variables predictoras)
#    - Patrones temporales no capturados por la configuraci√≥n autom√°tica
# 
# 3. **An√°lisis Adicional**:
#    - Componentes de tendencia y estacionalidad no mostraron patrones claros
#    - Intervalos de incertidumbre excesivamente amplios

# %% [markdown]
# ### Modelos Avanzados: LightGBM, XGBoost y Random Forest (Multivariados)

# %%
# Extraer columnas objetivo
target_cols = ['total_cost', 'net_profit']
X_selected = merged_df_reduced.drop(columns=target_cols)

# Eliminar columnas datetime si existen
X_model = X_selected.select_dtypes(exclude=['datetime', 'datetime64[ns]', 'object'])

# Bucle para entrenar y evaluar cada columna objetivo
for target_col in target_cols:
    y = merged_df_reduced[target_col]
    
    # Inicializar modelos
    models = {
        "LightGBM": LGBMRegressor(),
        "XGBoost": XGBRegressor(),
        "Random Forest": RandomForestRegressor()
    }
    
    # Configurar validaci√≥n cruzada
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    
    # Diccionario para almacenar m√©tricas
    eval_metrics = {
        "MAE": [],
        "RMSE": [],
        "R¬≤": []
    }
    
    # Validaci√≥n cruzada
    for model_name, model in models.items():
        print(f"üîß {model_name} para {target_col}:")
        
        mae_scores = []
        rmse_scores = []
        r2_scores = []
        
        # Iterar sobre los folds
        for train_idx, test_idx in cv.split(X_model, y):
            X_train, X_test = X_model.iloc[train_idx], X_model.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
            
            # Entrenar modelo
            model.fit(X_train, y_train)
            
            # Predicciones
            y_pred = model.predict(X_test)
            
            # Calcular m√©tricas
            mae_scores.append(mean_absolute_error(y_test, y_pred))
            rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))
            r2_scores.append(r2_score(y_test, y_pred))
        
        # Calcular promedios
        eval_metrics["MAE"].append(np.mean(mae_scores))
        eval_metrics["RMSE"].append(np.mean(rmse_scores))
        eval_metrics["R¬≤"].append(np.mean(r2_scores))
        
        # Mostrar resultados
        print(f"  - MAE Promedio: {np.mean(mae_scores):.2f}")
        print(f"  - RMSE Promedio: {np.mean(rmse_scores):.2f}")
        print(f"  - R¬≤ Promedio: {np.mean(r2_scores):.3f}")
        print("-" * 50)
    
    # Resumen para cada variable objetivo
    print(f"\n=========== Rendimiento Promedio para {target_col} ===========")
    print(f"  - MAE General: {np.mean(eval_metrics['MAE']):.2f}")
    print(f"  - RMSE General: {np.mean(eval_metrics['RMSE']):.2f}")
    print(f"  - R¬≤ General: {np.mean(eval_metrics['R¬≤']):.3f}")
    print("=" * 60)

# %% [markdown]
# **Resultados de Rendimiento**
# 
# **Costo Total**
# | Modelo           | MAE    | RMSE   | R¬≤    |
# |------------------|--------|--------|-------|
# | **Random Forest**| 107.26 | 225.47 | 0.996 |
# | **XGBoost**      | 208.13 | 276.92 | 0.993 |
# | **LightGBM**     | 219.30 | 299.68 | 0.992 |
# 
# **Beneficio Neto**
# | Modelo           | MAE    | RMSE   | R¬≤    |
# |------------------|--------|--------|-------|
# | **XGBoost**      | 83.90  | 114.53 | 0.993 |
# | **LightGBM**     | 85.14  | 121.98 | 0.992 |
# | **Random Forest**| 100.28 | 156.85 | 0.987 |
# 
# **M√©tricas Promedio**
# | M√©trica          | Costo Total | Beneficio Neto |
# |------------------|-------------|----------------|
# | **MAE General**  | 178.23      | 89.77          |
# | **RMSE General** | 267.36      | 131.12         |
# | **R¬≤ General**   | 0.994       | 0.991          |
# 
# **An√°lisis Detallado**
# 
# 1. **Random Forest**:
#    - **Fortalezas**: 
#      - Mejor rendimiento en costo total (R¬≤ 0.996)
#      - Robustez ante outliers
#      - MAE m√°s bajo en costo total (107.26)
#    - **Debilidades**:
#      - Mayor tiempo de entrenamiento
#      - Peor desempe√±o en beneficio neto (MAE 100.28)
# 
# 2. **XGBoost**:
#    - **Puntos Fuertes**:
#      - Mejor balance para beneficio neto (RMSE 114.53)
#      - Regularizaci√≥n incorporada
#      - Segundo mejor rendimiento en costo total
#    - **Limitaciones**:
#      - Sensible a hiperpar√°metros
#      - Requiere m√°s tuning
# 
# 3. **LightGBM**:
#    - **Ventajas**:
#      - Entrenamiento m√°s r√°pido
#      - Buen manejo de categor√≠as
#      - Rendimiento competitivo en beneficio neto
#    - **Desventajas**:
#      - Mayor RMSE en costo total (299.68)
#      - Propenso a overfitting sin par√°metros adecuados

# %% [markdown]
# ## Resumen de Selecci√≥n de Modelos
# 
# **Comparativa General de Modelos**
# 
# | Modelo          | Mejor para          | Ventajas Clave                     | Limitaciones                  |
# |-----------------|---------------------|------------------------------------|-------------------------------|
# | **SARIMA**      | -                   | Modelado temporal puro             | R¬≤ negativo (-0.001)          |
# | **Prophet**     | -                   | F√°cil interpretaci√≥n               | No captura relaciones complejas|
# | **Random Forest**| Costo Total         | Mayor precisi√≥n (R¬≤ 0.996)         | Computacionalmente costoso    |
# | **XGBoost**     | Beneficio Neto      | Mejor balance (RMSE 116.01)        | Sensible a hiperpar√°metros    |
# | **LightGBM**    | Implementaci√≥n r√°pida| Eficiencia computacional          | Requiere regularizaci√≥n       |
# 
# **Conclusiones Definitivas**
# 
# 1. **Para Costo Total**:
#    - **Random Forest** es el claro ganador
#    - Supera en 0.004 puntos de R¬≤ a XGBoost
#    - Aceptable tiempo de entrenamiento (3-5 mins)
# 
# 2. **Para Beneficio Neto**:
#    - **XGBoost** ofrece mejor equilibrio
#    - 15% mejor RMSE que LightGBM
#    - Permite mayor ajuste fino
# 
# 3. **Modelos Descartados**:
#    - SARIMA y Prophet mostraron R¬≤ negativos
#    - No justifican su implementaci√≥n

# %% [markdown]
# # Optimizaci√≥n del Modelo y Aprendizaje en Conjunto

# %% [markdown]
# ### Ajuste de Modelos y Guardado de los Mejores Modelos

# %%
# Modelo Random Forest
rf_model = RandomForestRegressor(random_state=42)

# Definici√≥n de la malla de hiperpar√°metros para RandomizedSearchCV - Random Forest
rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Modelo XGBoost sin aceleraci√≥n por GPU
xgb_model = XGBRegressor(random_state=42)

# Definici√≥n de la malla de hiperpar√°metros para RandomizedSearchCV - XGBoost
xgb_param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.7, 0.8, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0],
    'gamma': [0, 0.1, 0.2],
    'reg_alpha': [0, 0.1, 0.2],
    'reg_lambda': [0.1, 0.2, 0.3],
}

# Modelo LightGBM
lgbm_model = LGBMRegressor(random_state=42)

# Definici√≥n de la malla de hiperpar√°metros para RandomizedSearchCV - LGBM
lgbm_param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.7, 0.8, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0],
    'reg_alpha': [0, 0.1, 0.2],
    'reg_lambda': [0.1, 0.2, 0.3],
}

# Uso de un solo n√∫cleo para evitar problemas de memoria
n_jobs = 1  # Modo seguro para evitar errores por uso excesivo de memoria

# Random Forest - B√∫squeda aleatoria de hiperpar√°metros
rf_search = RandomizedSearchCV(rf_model, rf_param_grid, n_iter=10, cv=3, verbose=2, random_state=42, n_jobs=n_jobs)
rf_search.fit(X_train, y_train)

# Mejores hiperpar√°metros y desempe√±o - Random Forest
print(f"Mejores par√°metros Random Forest: {rf_search.best_params_}")
print(f"Mejor puntuaci√≥n Random Forest (CV): {rf_search.best_score_}")

# Evaluaci√≥n Random Forest
y_pred_rf = rf_search.predict(X_test)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)
print(f"RMSE Random Forest: {rmse_rf}")
print(f"R¬≤ Random Forest: {r2_rf}")

# Visualizaci√≥n Real vs Predicci√≥n - Random Forest
plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test, y=y_pred_rf)
plt.title('Valores reales vs Predicciones - Random Forest')
plt.xlabel('Valores Reales')
plt.ylabel('Predicciones')
plt.show()

# Guardar modelo Random Forest
dump(rf_search.best_estimator_, 'best_rf_model.pkl')

# Importancia de caracter√≠sticas - Random Forest
plt.figure(figsize=(10, 6))
sns.barplot(x=rf_search.best_estimator_.feature_importances_, y=X_train.columns)
plt.title('Importancia de Caracter√≠sticas - Random Forest')
plt.xlabel('Importancia')
plt.ylabel('Caracter√≠sticas')
plt.show()

# XGBoost - B√∫squeda aleatoria de hiperpar√°metros
xgb_search = RandomizedSearchCV(xgb_model, xgb_param_grid, n_iter=10, cv=3, verbose=2, random_state=42, n_jobs=n_jobs)
xgb_search.fit(X_train, y_train)

# Mejores hiperpar√°metros y desempe√±o - XGBoost
print(f"Mejores par√°metros XGBoost: {xgb_search.best_params_}")
print(f"Mejor puntuaci√≥n XGBoost (CV): {xgb_search.best_score_}")

# Evaluaci√≥n XGBoost
y_pred_xgb = xgb_search.predict(X_test)
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
r2_xgb = r2_score(y_test, y_pred_xgb)
print(f"RMSE XGBoost: {rmse_xgb}")
print(f"R¬≤ XGBoost: {r2_xgb}")

# Visualizaci√≥n Real vs Predicci√≥n - XGBoost
plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test, y=y_pred_xgb)
plt.title('Valores reales vs Predicciones - XGBoost')
plt.xlabel('Valores Reales')
plt.ylabel('Predicciones')
plt.show()

# Guardar modelo XGBoost
dump(xgb_search.best_estimator_, 'best_xgb_model.pkl')

# Importancia de caracter√≠sticas - XGBoost
plt.figure(figsize=(10, 6))
sns.barplot(x=xgb_search.best_estimator_.feature_importances_, y=X_train.columns)
plt.title('Importancia de Caracter√≠sticas - XGBoost')
plt.xlabel('Importancia')
plt.ylabel('Caracter√≠sticas')
plt.show()

# LGBM - B√∫squeda aleatoria de hiperpar√°metros
lgbm_search = RandomizedSearchCV(lgbm_model, lgbm_param_grid, n_iter=10, cv=3, verbose=2, random_state=42, n_jobs=n_jobs)
lgbm_search.fit(X_train, y_train)

# Mejores hiperpar√°metros y desempe√±o - LGBM
print(f"Mejores par√°metros LGBM: {lgbm_search.best_params_}")
print(f"Mejor puntuaci√≥n LGBM (CV): {lgbm_search.best_score_}")

# Evaluaci√≥n LGBM
y_pred_lgbm = lgbm_search.predict(X_test)
rmse_lgbm = np.sqrt(mean_squared_error(y_test, y_pred_lgbm))
r2_lgbm = r2_score(y_test, y_pred_lgbm)
print(f"RMSE LGBM: {rmse_lgbm}")
print(f"R¬≤ LGBM: {r2_lgbm}")

# Visualizaci√≥n Real vs Predicci√≥n - LGBM
plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test, y=y_pred_lgbm)
plt.title('Valores reales vs Predicciones - LGBM')
plt.xlabel('Valores Reales')
plt.ylabel('Predicciones')
plt.show()

# Guardar modelo LGBM
dump(lgbm_search.best_estimator_, 'best_lgbm_model.pkl')

# Importancia de caracter√≠sticas - LGBM
plt.figure(figsize=(10, 6))
sns.barplot(x=lgbm_search.best_estimator_.feature_importances_, y=X_train.columns)
plt.title('Importancia de Caracter√≠sticas - LGBM')
plt.xlabel('Importancia')
plt.ylabel('Caracter√≠sticas')
plt.show()


# %% [markdown]
# **Ajuste de Modelos y Guardado de los Mejores Modelos**
# 
# Esta secci√≥n se enfoca en la optimizaci√≥n de hiperpar√°metros y el almacenamiento de los modelos m√°s precisos utilizando tres algoritmos de regresi√≥n de conjunto:
# 
# - **Random Forest**
# - **XGBoost**
# - **LightGBM**
# 
# El ajuste de hiperpar√°metros se realiz√≥ mediante `RandomizedSearchCV` con validaci√≥n cruzada de 3 pliegues y procesamiento paralelo (`backend='loky'`) para acelerar la b√∫squeda.
# 
# ---
# 
# **Resultados del Ajuste de Modelos**
# 
# | Modelo         | RMSE     | R¬≤     | Mejores Hiperpar√°metros                             |
# |----------------|----------|--------|-----------------------------------------------------|
# | üå≤ Random Forest | 411.75   | 0.912  | `n_estimators=200`, `max_depth=7`, `bootstrap=True` |
# | ‚ö° XGBoost       | **93.02** | **0.996** | `n_estimators=200`, `max_depth=7`, `learning_rate=0.1` |
# | üí° LightGBM      | 97.51    | 0.995  | `n_estimators=300`, `max_depth=7`, `learning_rate=0.1` |
# 
# ---
# 
# **Visualizaciones Generadas**
# 
# - Predicci√≥n vs. Valor Real (scatter plots)
# - Importancia de caracter√≠sticas
# - Comparaciones visuales entre modelos
# 
# Los modelos entrenados y ajustados fueron guardados como archivos `.pkl` para su posterior implementaci√≥n o an√°lisis.
# 
# ---
# 
# **Conclusiones**
# 
# - **XGBoost** logr√≥ el mejor rendimiento general, destac√°ndose por su alta precisi√≥n y bajo error.
# - **LightGBM** fue una alternativa s√≥lida con resultados muy cercanos.
# - **Random Forest**, aunque robusto, qued√≥ por detr√°s en m√©tricas clave.
# - La optimizaci√≥n de hiperpar√°metros mejor√≥ significativamente el rendimiento de todos los modelos.
# 
# **XGBoost es el principal candidato para el despliegue en producci√≥n.**
# 

# %% [markdown]
# ### Modelado en Conjunto con Visualizaci√≥n

# %%
# Cargar los mejores modelos previamente entrenados
best_xgb_model = joblib.load('best_xgb_model.pkl')
best_lgbm_model = joblib.load('best_lgbm_model.pkl')

# Definir los modelos base para el stacking
base_models = [
    ('xgb', best_xgb_model),
    ('lgbm', best_lgbm_model)
]

# Definir el modelo meta
stacking_model = StackingRegressor(estimators=base_models, final_estimator=LinearRegression())

# Entrenar el modelo de stacking
stacking_model.fit(X_train, y_train)

# Guardar el modelo de stacking
dump(stacking_model, 'best_stacking_model.pkl')

# Evaluaci√≥n con validaci√≥n cruzada
stacking_scores = cross_val_score(stacking_model, X_train, y_train, cv=3, scoring='neg_mean_squared_error')
stacking_rmse = np.mean(np.sqrt(-stacking_scores))
print(f"RMSE promedio del modelo Stacking: {stacking_rmse:.2f}")

# Visualizaci√≥n: Valores Reales vs Predichos
y_pred_stacking = stacking_model.predict(X_test)
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_stacking, color='blue', label='Predicho vs Real')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', lw=2, label='Predicci√≥n Ideal')
plt.xlabel('Valores Reales')
plt.ylabel('Valores Predichos')
plt.title('Modelo Stacking: Predicci√≥n vs Realidad')
plt.legend()
plt.grid(True)
plt.show()

# Guardar las predicciones para an√°lisis futuro
dump(y_pred_stacking, 'stacking_model_predictions.pkl')


# %% [markdown]
# **Modelado en Conjunto con Visualizaci√≥n**
# 
# En esta etapa, se implement√≥ un modelo de ensamble tipo **Stacking Regressor** para combinar el poder predictivo de los dos mejores modelos: **XGBoost** y **LightGBM**, utilizando una **Regresi√≥n Lineal** como modelo meta (*meta-learner*).
# 
# El objetivo fue mejorar la precisi√≥n general del pron√≥stico al aprovechar la complementariedad de los modelos base.
# 
# ---
# 
# **Configuraci√≥n del Modelo de Ensamble**
# 
# - **Modelos base**: XGBoost + LightGBM
# - **Modelo meta**: Regresi√≥n Lineal
# - **Validaci√≥n cruzada**: 3 pliegues
# - **M√©trica de evaluaci√≥n**: RMSE (Root Mean Squared Error)
# 
# ---
# 
# **Resultados del Modelo de Stacking**
# 
# | M√©trica        | Valor     |
# |----------------|-----------|
# | RMSE Promedio | **92.52** |
# 
# - El gr√°fico de valores predichos vs. reales mostr√≥ una relaci√≥n lineal fuerte, lo que sugiere alta precisi√≥n del modelo.
# - Las predicciones fueron almacenadas para an√°lisis posteriores.
# - El modelo ensamblado fue guardado para su implementaci√≥n futura (`.pkl`).
# 
# ---
# 
# **Conclusiones**
# 
# - El **modelo de stacking** logr√≥ mejorar el rendimiento respecto a los modelos individuales.
# - Aprovech√≥ la **fortaleza de XGBoost en precisi√≥n** y la **eficiencia de LightGBM**.
# - La regresi√≥n lineal como meta-modelo permiti√≥ combinar eficazmente sus predicciones.
# 
# **El modelo en conjunto es una excelente opci√≥n para producci√≥n y toma de decisiones basadas en pron√≥sticos financieros.**
# 

# %% [markdown]
# ### Voting Regressor con Visualizaci√≥n

# %%
# Cargar los mejores modelos previamente entrenados
best_xgb_model = joblib.load('best_xgb_model.pkl')
best_lgbm_model = joblib.load('best_lgbm_model.pkl')

# Inicializar el modelo de regresi√≥n por votaci√≥n con los mejores modelos
voting_model = VotingRegressor(estimators=[
    ('xgb', best_xgb_model),
    ('lgbm', best_lgbm_model)
])

# Entrenar el modelo de votaci√≥n
voting_model.fit(X_model, y)

# Guardar el modelo de votaci√≥n para su posterior despliegue
joblib.dump(voting_model, 'best_voting_model.pkl')

# Evaluar el modelo de votaci√≥n usando validaci√≥n cruzada
voting_scores = cross_val_score(voting_model, X_model, y, cv=3, scoring='neg_mean_squared_error')
voting_rmse = np.mean(np.sqrt(-voting_scores))

print(f"RMSE promedio del modelo Voting: {voting_rmse:.2f}")

# Visualizaci√≥n para el modelo de votaci√≥n
y_pred_voting = voting_model.predict(X_test)
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_voting, color='green', label='Predicho vs Real')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', lw=2, label='Predicci√≥n Ideal')
plt.xlabel('Valores Reales')
plt.ylabel('Valores Predichos')
plt.title('Modelo Voting: Predicci√≥n vs Realidad')
plt.legend()
plt.grid(True)
plt.show()

# Guardar las predicciones del modelo de votaci√≥n para an√°lisis posterior si es necesario
joblib.dump(y_pred_voting, 'voting_model_predictions.pkl')


# %% [markdown]
# **Voting Regressor con Visualizaci√≥n**
# 
# En esta etapa, se implement√≥ un modelo de ensamble utilizando la t√©cnica **Voting Regressor**, que combina las predicciones de dos modelos previamente optimizados: **XGBoost** y **LightGBM**.
# 
# El objetivo fue mejorar la robustez del modelo mediante la agregaci√≥n de predicciones, aprovechando la estabilidad conjunta de ambos modelos.
# 
# ---
# 
# **Configuraci√≥n del Voting Regressor**
# 
# - **Modelos incluidos**: XGBoost + LightGBM
# - **Tipo de votaci√≥n**: Promedio de predicciones
# - **Validaci√≥n cruzada**: 3 pliegues
# - **M√©trica de evaluaci√≥n**: RMSE (Root Mean Squared Error)
# 
# ---
# 
# **Resultados del Ensamble por Votaci√≥n**
# 
# | M√©trica        | Valor     |
# |----------------|-----------|
# | RMSE Promedio | **88.03** |
# 
# - El gr√°fico de valores reales vs. predichos mostr√≥ que el modelo sigue de cerca la l√≠nea ideal de predicci√≥n, indicando una excelente capacidad de ajuste.
# - El modelo fue guardado como archivo `.pkl` para uso futuro en producci√≥n.
# 
# ---
# 
# **Conclusiones**
# 
# - El **Voting Regressor** ofreci√≥ un rendimiento s√≥lido, con **bajo error promedio y alta estabilidad**.
# - La combinaci√≥n de **XGBoost y LightGBM** captur√≥ lo mejor de ambos enfoques de boosting.
# - Es una excelente opci√≥n para ser usada en producci√≥n, especialmente cuando se busca **un equilibrio entre precisi√≥n y robustez**.
# 
# **Recomendado como alternativa estable al modelo de Stacking para pron√≥sticos financieros.**
# 

# %% [markdown]
# #### An√°lisis de Importancia de las Caracter√≠sticas

# %%
# ----------------------------- Preprocesamiento -----------------------------
def preprocess_data(X, fit=False, imputers=None, encoders=None):
    X_copy = X.copy()
    object_cols = X_copy.select_dtypes(include='object').columns
    num_cols = X_copy.select_dtypes(exclude='object').columns

    if fit:
        imputers = {
            'num': SimpleImputer(strategy="mean"),
            'cat': SimpleImputer(strategy="most_frequent") if len(object_cols) > 0 else None
        }

        if len(num_cols) > 0:
            X_copy[num_cols] = imputers['num'].fit_transform(X_copy[num_cols])
        if len(object_cols) > 0:
            X_copy[object_cols] = imputers['cat'].fit_transform(X_copy[object_cols])

        encoders = {}
        for col in object_cols:
            le = LabelEncoder()
            X_copy[col] = le.fit_transform(X_copy[col].astype(str))
            encoders[col] = le

    else:
        if len(num_cols) > 0:
            X_copy[num_cols] = imputers['num'].transform(X_copy[num_cols])
        if len(object_cols) > 0:
            X_copy[object_cols] = imputers['cat'].transform(X_copy[object_cols])
            for col in object_cols:
                le = encoders[col]
                X_copy[col] = le.transform(X_copy[col].astype(str))

    return X_copy, imputers, encoders

# ----------------------------- Cargar modelos -----------------------------
print("Cargando modelos...")
best_xgb_model = joblib.load('best_xgb_model.pkl')
best_lgbm_model = joblib.load('best_lgbm_model.pkl')

# ----------------------------- Preparar los datos -----------------------------
print("Preprocesando los datos...")
X_model_numeric, imputers, encoders = preprocess_data(X_model, fit=True)
X_test_numeric, _, _ = preprocess_data(X_test, fit=False, imputers=imputers, encoders=encoders)
X_test_numeric = X_test_numeric.reindex(columns=X_model_numeric.columns, fill_value=0)

# ----------------------------- SHAP para XGBoost -----------------------------
print("Generando explicaciones SHAP para XGBoost...")
explainer_xgb = shap.Explainer(best_xgb_model, X_model_numeric)
shap_values_xgb = explainer_xgb(X_model_numeric)

print("Generando gr√°fico resumen SHAP (XGBoost)...")
shap.summary_plot(shap_values_xgb, X_model_numeric, plot_type="bar", show=False)
plt.title("Resumen SHAP - XGBoost")
plt.savefig('shap_summary_xgb.png')
plt.close()

print("Top 10 caracter√≠sticas m√°s importantes (XGBoost):")
mean_shap_xgb = pd.DataFrame({
    'Feature': X_model_numeric.columns,
    'Importance': abs(shap_values_xgb.values).mean(axis=0)
}).sort_values(by='Importance', ascending=False).head(10)
print(mean_shap_xgb)

# ----------------------------- SHAP para LightGBM -----------------------------
print("Generando explicaciones SHAP para LightGBM...")
explainer_lgbm = shap.Explainer(best_lgbm_model, X_model_numeric)
shap_values_lgbm = explainer_lgbm(X_model_numeric, check_additivity=False)

print("Generando gr√°fico resumen SHAP (LightGBM)...")
shap.summary_plot(shap_values_lgbm, X_model_numeric, plot_type="bar", show=False)
plt.title("Resumen SHAP - LightGBM")
plt.savefig('shap_summary_lgbm.png')
plt.close()

print("Top 10 caracter√≠sticas m√°s importantes (LightGBM):")
mean_shap_lgbm = pd.DataFrame({
    'Feature': X_model_numeric.columns,
    'Importance': abs(shap_values_lgbm.values).mean(axis=0)
}).sort_values(by='Importance', ascending=False).head(10)
print(mean_shap_lgbm)

# ----------------------------- SHAP Force Plot -----------------------------
print("Generando gr√°fico de fuerza SHAP para la primera fila (XGBoost)...")
force_plot_xgb = shap.plots.force(explainer_xgb(X_model_numeric.iloc[0]), matplotlib=True, show=False)
plt.title("SHAP Force Plot - XGBoost (Fila 0)")
plt.savefig('shap_force_plot_xgb.png')
plt.close()

# ----------------------------- Interfaz en Streamlit -----------------------------
st.title("üîç An√°lisis de Importancia de Variables con SHAP")

st.subheader("Importancia de Variables - XGBoost")
st.image('shap_summary_xgb.png', caption='Resumen SHAP XGBoost (Top Caracter√≠sticas)', use_column_width=True)

st.subheader("Importancia de Variables - LightGBM")
st.image('shap_summary_lgbm.png', caption='Resumen SHAP LightGBM (Top Caracter√≠sticas)', use_column_width=True)

st.subheader("SHAP Force Plot - Primera Predicci√≥n con XGBoost")
st.image('shap_force_plot_xgb.png', caption='Gr√°fico de Fuerza para la Primera Fila', use_column_width=True)

st.subheader("Top 10 Variables m√°s importantes seg√∫n XGBoost")
st.dataframe(mean_shap_xgb.reset_index(drop=True))

st.subheader("Top 10 Variables m√°s importantes seg√∫n LightGBM")
st.dataframe(mean_shap_lgbm.reset_index(drop=True))


# %% [markdown]
# **An√°lisis de Importancia de las Caracter√≠sticas**
# 
# Se utiliz√≥ an√°lisis de **importancia de caracter√≠sticas** y visualizaciones SHAP para entender qu√© variables tienen mayor impacto en las predicciones de los modelos entrenados (XGBoost y LightGBM).
# 
# ---
# 
# **Principales Caracter√≠sticas Identificadas**
# 
# Las variables m√°s influyentes en ambos modelos fueron:
# 
# 1. **`profit_margin_percentage`** ‚Äì la caracter√≠stica m√°s importante en ambos modelos.
# 2. **`transportation_cost_percent`**
# 3. **`transportation_cost`**
# 4. **`regulatory_fees_percent`**
# 5. **`regulatory_fees`**
# 6. **`cost_anomaly`**
# 
# Estas variables est√°n directamente relacionadas con los costos operativos y la rentabilidad, lo que justifica su alto nivel de influencia en las predicciones.
# 
# ---
# 
# **Conclusi√≥n**
# 
# - `profit_margin_percentage` sobresale como la variable m√°s determinante para ambos objetivos: **costo total** y **ganancia neta**.
# - El an√°lisis SHAP permiti√≥ una interpretaci√≥n clara y transparente, fundamental para generar confianza en entornos de negocio.
# - Estas visualizaciones facilitan la validaci√≥n del modelo con partes interesadas no t√©cnicas y respaldan la toma de decisiones basada en datos.
# 
# **La interpretabilidad del modelo es clave para su adopci√≥n en ambientes corporativos.**
# 

# %% [markdown]
# ### An√°lisis de errores

# %%
# Cargar modelos entrenados
print("Cargando los modelos entrenados...")
best_xgb_model = joblib.load('best_xgb_model.pkl')
best_lgbm_model = joblib.load('best_lgbm_model.pkl')

# Generar predicciones sobre los datos de prueba
print("Generando predicciones sobre los datos de prueba...")
y_pred_xgb = best_xgb_model.predict(X_test_numeric)
y_pred_lgbm = best_lgbm_model.predict(X_test_numeric)

# Valores reales (valores verdaderos)
y_true = y_test

# Calcular los residuos
print("Calculando los residuos...")
residuals_xgb = y_true - y_pred_xgb
residuals_lgbm = y_true - y_pred_lgbm

# --- M√©tricas de error ---
print("\nCalculando las m√©tricas de error...")
mae_xgb = mean_absolute_error(y_true, y_pred_xgb)
mae_lgbm = mean_absolute_error(y_true, y_pred_lgbm)
mse_xgb = mean_squared_error(y_true, y_pred_xgb)
mse_lgbm = mean_squared_error(y_true, y_pred_lgbm)
r2_xgb = r2_score(y_true, y_pred_xgb)
r2_lgbm = r2_score(y_true, y_pred_lgbm)

print(f"\nüìä Rendimiento del modelo XGBoost:")
print(f"  - MAE: {mae_xgb:.2f}")
print(f"  - MSE: {mse_xgb:.2f}")
print(f"  - R2 Score: {r2_xgb:.4f}")

print(f"\nüìä Rendimiento del modelo LightGBM:")
print(f"  - MAE: {mae_lgbm:.2f}")
print(f"  - MSE: {mse_lgbm:.2f}")
print(f"  - R2 Score: {r2_lgbm:.4f}")

# --- Visualizar distribuciones de residuos ---
print("\nVisualizando distribuciones de los residuos...")
plt.figure(figsize=(10, 6))
sns.histplot(residuals_xgb, kde=True, color='blue', label='Residuos de XGBoost')
plt.title('Distribuci√≥n de los residuos para XGBoost')
plt.xlabel('Residuos')
plt.ylabel('Frecuencia')
plt.legend()
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(residuals_lgbm, kde=True, color='green', label='Residuos de LightGBM')
plt.title('Distribuci√≥n de los residuos para LightGBM')
plt.xlabel('Residuos')
plt.ylabel('Frecuencia')
plt.legend()
plt.show()

# --- Comparar errores en cada predicci√≥n ---
print("\nComparando los 10 errores m√°s grandes...")
error_comparison = pd.DataFrame({
    'Valor Real': y_true,
    'Predicci√≥n XGBoost': y_pred_xgb,
    'Predicci√≥n LightGBM': y_pred_lgbm,
    'Residuos XGBoost': residuals_xgb,
    'Residuos LightGBM': residuals_lgbm
})

top_errors_xgb = error_comparison.iloc[np.argsort(np.abs(residuals_xgb))[-10:]]
top_errors_lgbm = error_comparison.iloc[np.argsort(np.abs(residuals_lgbm))[-10:]]

print("\nüîç Los 10 errores m√°s grandes para XGBoost:")
print(top_errors_xgb)

print("\nüîç Los 10 errores m√°s grandes para LightGBM:")
print(top_errors_lgbm)

# --- An√°lisis de importancia de caracter√≠sticas y SHAP ---
print("\nCalculando los valores SHAP e importancia de las caracter√≠sticas para XGBoost...")
explainer_xgb = shap.Explainer(best_xgb_model, X_test_numeric)
shap_values_xgb = explainer_xgb(X_test_numeric)

print("Mostrando el gr√°fico de resumen de SHAP para XGBoost...")
shap.summary_plot(shap_values_xgb, X_test_numeric, plot_type="bar")
plt.show()

print("\nCalculando los valores SHAP e importancia de las caracter√≠sticas para LightGBM...")
explainer_lgbm = shap.Explainer(best_lgbm_model, X_test_numeric)
shap_values_lgbm = explainer_lgbm(X_test_numeric, check_additivity=False)

print("Mostrando el gr√°fico de resumen de SHAP para LightGBM...")
shap.summary_plot(shap_values_lgbm, X_test_numeric, plot_type="bar")
plt.show()

# --- Visualizar SHAP vs Residuos ---
print("\nAnalizando la relaci√≥n entre los valores SHAP y los residuos...")

shap_values_xgb_df = pd.DataFrame(shap_values_xgb.values, columns=X_test_numeric.columns)
top_feature_xgb = shap_values_xgb_df.abs().mean().idxmax()
print(f"La caracter√≠stica m√°s relevante en XGBoost: {top_feature_xgb}")

plt.figure(figsize=(10, 6))
plt.scatter(shap_values_xgb_df[top_feature_xgb], residuals_xgb, alpha=0.6, color='blue')
plt.title(f'Residuales vs valores SHAP para {top_feature_xgb} (XGBoost)')
plt.xlabel(f'Valores SHAP para {top_feature_xgb}')
plt.ylabel('Residuos')
plt.show()

shap_values_lgbm_df = pd.DataFrame(shap_values_lgbm.values, columns=X_test_numeric.columns)
top_feature_lgbm = shap_values_lgbm_df.abs().mean().idxmax()
print(f"La caracter√≠stica m√°s relevante en LightGBM: {top_feature_lgbm}")

plt.figure(figsize=(10, 6))
plt.scatter(shap_values_lgbm_df[top_feature_lgbm], residuals_lgbm, alpha=0.6, color='green')
plt.title(f'Residuales vs valores SHAP para {top_feature_lgbm} (LightGBM)')
plt.xlabel(f'Valores SHAP para {top_feature_lgbm}')
plt.ylabel('Residuos')
plt.show()

# --- Errores grandes basados en el umbral ---
threshold_grande_error = 10000
print(f"\nBuscando residuos mayores que ¬±{threshold_grande_error}...")

grandes_errores_xgb = error_comparison[error_comparison['Residuos XGBoost'].abs() > threshold_grande_error]
grandes_errores_lgbm = error_comparison[error_comparison['Residuos LightGBM'].abs() > threshold_grande_error]

print(f"\n‚ö†Ô∏è Predicciones de XGBoost con residuos > ¬±{threshold_grande_error}:")
print(grandes_errores_xgb)

print(f"\n‚ö†Ô∏è Predicciones de LightGBM con residuos > ¬±{threshold_grande_error}:")
print(grandes_errores_lgbm)


# %% [markdown]
# **An√°lisis de Errores**
# 
# Se realiz√≥ un an√°lisis detallado de los errores de predicci√≥n de los modelos **XGBoost** y **LightGBM**, con el objetivo de evaluar su estabilidad, detectar posibles patrones de fallo y entender mejor sus limitaciones.
# 
# ---
# 
# **M√©tricas de Rendimiento**
# 
# | Modelo     | MAE     | MSE        | R¬≤     |
# |------------|---------|------------|--------|
# | ‚ö° XGBoost  | 63.56   | 8,652.25   | 0.9955 |
# | üí° LightGBM | 68.69   | 9,508.12   | 0.9951 |
# 
# - **XGBoost** super√≥ ligeramente a LightGBM en todas las m√©tricas.
# - Ambos modelos muestran una capacidad de predicci√≥n excepcional, con errores absolutos bajos y alta precisi√≥n.
# 
# ---
# 
# **Distribuci√≥n de Errores**
# 
# - Los residuos de ambos modelos est√°n **centrados alrededor de cero**, lo que indica que no hay sesgo sistem√°tico en las predicciones.
# - **No se detectaron valores extremos graves** (errores superiores a ¬±10.000), lo que confirma una buena estabilidad general.
# - Los histogramas y gr√°ficos de residuos no muestran patrones preocupantes.
# 
# ---
# 
# **Principales Casos con Alto Error**
# 
# - Se identificaron los **10 registros con mayor error** en cada modelo.
# - Algunos registros se repiten entre ambos modelos, lo cual sugiere que existen observaciones inherentemente dif√≠ciles de predecir.
# - En estos casos, las diferencias entre valores reales y predichos **no superaron los ¬±1.000**, lo que sigue siendo aceptable para un entorno financiero.
# 
# ---
# 
# **An√°lisis SHAP aplicado a los errores**
# 
# - La variable m√°s influyente en los errores fue nuevamente **`profit_margin_percentage`**.
# - Se detect√≥ una **relaci√≥n moderada** entre valores extremos de esta variable y errores m√°s altos.
# - Esto sugiere que **casos con m√°rgenes de beneficio at√≠picos** tienden a ser m√°s dif√≠ciles de predecir con precisi√≥n.
# 
# ---
# 
# **Conclusi√≥n**
# 
# - Ambos modelos presentan **excelente desempe√±o predictivo** y gran estabilidad.
# - **XGBoost** es el modelo preferido, ya que mantiene un menor MAE y mejor ajuste general.
# - Se recomienda:
#   - Continuar refinando los modelos para casos dif√≠ciles.
#   - Monitorear observaciones con `profit_margin_percentage` extremos.
#   - Aplicar validaciones adicionales si se utilizan estos modelos en producci√≥n.
# 
# **Este an√°lisis valida la robustez de los modelos y destaca oportunidades para ajustes finos.**
# 

# %% [markdown]
# #### Comprobaci√≥n de la cordura del modelo de pron√≥stico

# %%
# Cargar los modelos entrenados
print("üîÑ Cargando los modelos entrenados...")
best_xgb_model = joblib.load('best_xgb_model.pkl')
best_lgbm_model = joblib.load('best_lgbm_model.pkl')

# Datos de prueba
X_test = X_test_numeric
y_true = y_test

# Hacer predicciones
print("üìà Realizando predicciones...")
y_pred_xgb = best_xgb_model.predict(X_test)
y_pred_lgbm = best_lgbm_model.predict(X_test)

# --- 1. Evaluar el rendimiento del modelo ---
print("\nüìä M√©tricas de rendimiento del modelo:")
mae_xgb = mean_absolute_error(y_true, y_pred_xgb)
mae_lgbm = mean_absolute_error(y_true, y_pred_lgbm)
mse_xgb = mean_squared_error(y_true, y_pred_xgb)
mse_lgbm = mean_squared_error(y_true, y_pred_lgbm)
r2_xgb = r2_score(y_true, y_pred_xgb)
r2_lgbm = r2_score(y_true, y_pred_lgbm)

print(f"XGBoost - MAE: {mae_xgb:.4f}, MSE: {mse_xgb:.4f}, R¬≤: {r2_xgb:.4f}")
print(f"LightGBM - MAE: {mae_lgbm:.4f}, MSE: {mse_lgbm:.4f}, R¬≤: {r2_lgbm:.4f}")

# --- 2. Validaci√≥n cruzada ---
print("\nüîç Realizando validaci√≥n cruzada de 5 pliegues...")
cv_score_xgb = cross_val_score(best_xgb_model, X_test, y_true, cv=5, scoring='neg_mean_absolute_error')
cv_score_lgbm = cross_val_score(best_lgbm_model, X_test, y_true, cv=5, scoring='neg_mean_absolute_error')

print(f"XGBoost CV MAE promedio: {-np.mean(cv_score_xgb):.4f}")
print(f"LightGBM CV MAE promedio: {-np.mean(cv_score_lgbm):.4f}")

# --- 3. Comparaci√≥n con el modelo base ---
print("\nüßÆ Comparando con el modelo base (predictor de la media)...")
baseline_prediction = np.mean(y_true)
mae_baseline = mean_absolute_error(y_true, [baseline_prediction] * len(y_true))

print(f"MAE del modelo base: {mae_baseline:.4f}")
print(f"Mejora de XGBoost sobre el modelo base: {mae_baseline - mae_xgb:.4f}")
print(f"Mejora de LightGBM sobre el modelo base: {mae_baseline - mae_lgbm:.4f}")

# --- 4. Graficar pron√≥sticos vs valores reales ---
print("\nüìâ Graficando los pron√≥sticos vs los valores reales...")
plt.figure(figsize=(10, 6))
plt.plot(y_true, label='Valores Reales', color='blue')
plt.plot(y_pred_xgb, label='Pron√≥stico XGBoost', linestyle='dashed', color='orange')
plt.plot(y_pred_lgbm, label='Pron√≥stico LightGBM', linestyle='dashed', color='green')
plt.title('Pron√≥stico vs Valores Reales')
plt.xlabel('Tiempo')
plt.ylabel('Valor Objetivo')
plt.legend()
plt.show()

# --- 5. Intervalo de predicci√≥n ---
print("\nüîé Calculando y graficando los intervalos de predicci√≥n...")
pred_std_xgb = np.std(y_pred_xgb)
pred_std_lgbm = np.std(y_pred_lgbm)

lower_bound_xgb = y_pred_xgb - 1.96 * pred_std_xgb
upper_bound_xgb = y_pred_xgb + 1.96 * pred_std_xgb

lower_bound_lgbm = y_pred_lgbm - 1.96 * pred_std_lgbm
upper_bound_lgbm = y_pred_lgbm + 1.96 * pred_std_lgbm

print(f"Desviaci√≥n est√°ndar de la predicci√≥n de XGBoost: {pred_std_xgb:.2f}")
print(f"Desviaci√≥n est√°ndar de la predicci√≥n de LightGBM: {pred_std_lgbm:.2f}")

plt.figure(figsize=(10, 6))
plt.plot(y_true, label='Valores Reales', color='blue')
plt.plot(y_pred_xgb, label='Pron√≥stico XGBoost', linestyle='dashed', color='orange')
plt.fill_between(range(len(y_true)), lower_bound_xgb, upper_bound_xgb, color='orange', alpha=0.2, label='Intervalo XGBoost')
plt.plot(y_pred_lgbm, label='Pron√≥stico LightGBM', linestyle='dashed', color='green')
plt.fill_between(range(len(y_true)), lower_bound_lgbm, upper_bound_lgbm, color='green', alpha=0.2, label='Intervalo LightGBM')
plt.title('Pron√≥sticos con Intervalos de Predicci√≥n')
plt.xlabel('Tiempo')
plt.ylabel('Valor Objetivo')
plt.legend()
plt.show()

# --- 6. An√°lisis de residuos ---
print("\nüìè Analizando los residuos para sesgo o patrones...")
residuals_xgb = y_true - y_pred_xgb
residuals_lgbm = y_true - y_pred_lgbm

print(f"Residuos promedio (XGBoost): {np.mean(residuals_xgb):.4f}")
print(f"Residuos promedio (LightGBM): {np.mean(residuals_lgbm):.4f}")

plt.figure(figsize=(10, 6))
sns.histplot(residuals_xgb, kde=True, color='blue', label='Residuos de XGBoost')
sns.histplot(residuals_lgbm, kde=True, color='green', label='Residuos de LightGBM')
plt.title('Distribuci√≥n de los Residuos')
plt.xlabel('Residuos')
plt.ylabel('Frecuencia')
plt.legend()
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(y_pred_xgb, residuals_xgb, color='blue', alpha=0.6, label='XGBoost')
plt.scatter(y_pred_lgbm, residuals_lgbm, color='green', alpha=0.6, label='LightGBM')
plt.axhline(0, color='red', linestyle='--')
plt.title('Residuos vs Predicciones')
plt.xlabel('Valores Predichos')
plt.ylabel('Residuos')
plt.legend()
plt.show()


# %% [markdown]
# **Comprobaci√≥n de la Cordura del Modelo de Pron√≥stico**
# 
# Esta secci√≥n eval√∫a si los modelos de pron√≥stico (XGBoost y LightGBM) son l√≥gicamente consistentes y robustos a trav√©s de m√©tricas clave, validaci√≥n cruzada, an√°lisis residual y comparaci√≥n contra un modelo base.
# 
# ---
# 
# **M√©tricas de Rendimiento**
# 
# | Modelo     | MAE      | MSE         | R¬≤     |
# |------------|----------|-------------|--------|
# | ‚ö° XGBoost  | 63.56    | 8,652.25    | 0.9955 |
# | üí° LightGBM | 68.69    | 9,508.12    | 0.9951 |
# 
# - Ambos modelos presentan **altos valores de R¬≤**, lo que indica que explican la mayor√≠a de la varianza en los datos.
# - Los errores son bajos y est√°n en un rango aceptable para pron√≥sticos financieros.
# 
# ---
# 
# **Validaci√≥n Cruzada**
# 
# | Modelo     | MAE Promedio (CV) |
# |------------|-------------------|
# | ‚ö° XGBoost  | 102.16            |
# | üí° LightGBM | 97.75             |
# 
# - Se observa un **aumento en el MAE durante la validaci√≥n cruzada**, lo que sugiere un posible **overfitting leve** en los datos de entrenamiento.
# 
# ---
# 
# **Comparaci√≥n con el Modelo Base**
# 
# | M√©trica          | Valor      |
# |------------------|------------|
# | MAE (modelo base ‚Äì media) | 1,146.72  |
# | Mejora con XGBoost        | **1,083.16** |
# | Mejora con LightGBM       | **1,078.04** |
# 
# - Ambos modelos **superan ampliamente al predictor promedio**, lo que confirma su valor predictivo.
# 
# ---
# 
# **Intervalos de Predicci√≥n**
# 
# | Modelo     | Desviaci√≥n Est√°ndar de Predicci√≥n |
# |------------|------------------------------------|
# | ‚ö° XGBoost  | 1,383.28                           |
# | üí° LightGBM | 1,379.01                           |
# 
# - Aunque los modelos predicen con precisi√≥n, los **intervalos de predicci√≥n son amplios**, reflejando incertidumbre com√∫n en series temporales.
# 
# ---
# 
# **An√°lisis de Residuos**
# 
# | Modelo     | Residuo Medio |
# |------------|----------------|
# | ‚ö° XGBoost  | -1.84          |
# | üí° LightGBM | -0.78          |
# 
# - Ambos modelos presentan un **ligero sesgo negativo**, indicando que tienden a **subestimar ligeramente**.
# - Los gr√°ficos de residuos no muestran **patrones an√≥malos ni sesgos sistem√°ticos**.
# 
# ---
# 
# **Conclusi√≥n**
# 
# - Ambos modelos ofrecen **predicciones coherentes, precisas y robustas**.
# - **XGBoost** se mantiene como el modelo con mejor rendimiento global.
# - **Ligero overfitting detectado**, pero manejable y com√∫n en problemas reales.
# - Los resultados respaldan la **fiabilidad del modelo para implementaci√≥n en entornos productivos**.
# 
# **La comprobaci√≥n de cordura confirma que el modelo generaliza bien y supera ampliamente el enfoque base.**
# 

# %% [markdown]
# # Guardado de Modelos: Creaci√≥n de Directorio y Persistencia

# %%
import os
from joblib import dump

# Crear directorio si no existe
model_dir = "saved_models"
os.makedirs(model_dir, exist_ok=True)

# Guardar los modelos entrenados con los nombres usados previamente
dump(best_xgb_model, os.path.join(model_dir, 'best_xgb_model.pkl'))
dump(best_lgbm_model, os.path.join(model_dir, 'best_lgbm_model.pkl'))
dump(stacking_model, os.path.join(model_dir, 'best_stacking_model.pkl'))

# Guardar predicciones del modelo stacking
dump(y_pred_stacking, os.path.join(model_dir, 'stacking_model_predictions.pkl'))


# %% [markdown]
# ## ‚úÖ Conclusi√≥n del Proyecto
# 
# ### üìå Conclusi√≥n General  
# Este proyecto ha demostrado c√≥mo un enfoque basado en datos y aprendizaje autom√°tico puede transformar el proceso de planificaci√≥n presupuestaria de una empresa. A trav√©s de la integraci√≥n de m√∫ltiples fuentes de datos, el modelado predictivo y la visualizaci√≥n interactiva, hemos construido una soluci√≥n robusta, automatizada y explicable para estimar con precisi√≥n costos, ingresos y utilidades netas.
# 
# ---
# 
# ### üìä Resultados Clave
# - Se entrenaron y evaluaron m√∫ltiples modelos de machine learning (`Random Forest`, `LightGBM`, `XGBoost`) para predecir el **costo total** y la **utilidad neta**, logrando m√©tricas de precisi√≥n satisfactorias (ej. `MAE` < 5% del valor real).
# - Se logr√≥ identificar las variables m√°s influyentes en el desempe√±o financiero, como `service_type`, `waste_volume_tons`, `labor_cost` y `profit_margin_percentage`.
# - Se desarroll√≥ una app interactiva en Streamlit y una integraci√≥n con Power BI que permite visualizar en tiempo real los resultados y predicciones del modelo.
# 
# ---
# 
# ### üß© Desaf√≠os Resueltos
# - Se manejaron eficientemente datos faltantes, valores at√≠picos y formatos inconsistentes.
# - Se resolvi√≥ la fusi√≥n de datasets complejos mediante claves comunes (`service_id`, `service_date`).
# - Se construyeron pipelines modulares para facilitar futuras actualizaciones y mantenimiento del sistema.
# 
# ---
# 
# ### üìà Impacto en el Negocio
# - Permite **anticipar desviaciones presupuestarias**, mejorar la asignaci√≥n de recursos y tomar decisiones informadas con base en predicciones confiables.
# - Facilita una visi√≥n global de los impulsores del costo y la rentabilidad por tipo de servicio, zona geogr√°fica y cliente.
# - Reduce la dependencia de m√©todos manuales o est√°ticos como hojas de c√°lculo, proporcionando una **herramienta escalable y automatizada**.
# 
# ---
# 
# ### üéì Lecciones Aprendidas
# - La calidad y estructura de los datos iniciales son determinantes para el √©xito del modelo.
# - Modelos explicables (como LightGBM con SHAP) son cruciales para generar confianza en usuarios no t√©cnicos.
# - La separaci√≥n clara entre etapas (EDA, modelado, visualizaci√≥n) acelera el desarrollo y la validaci√≥n.
# 
# ---
# 
# ### üåç Aplicaci√≥n en el Mundo Real
# - Empresas de servicios, log√≠stica, reciclaje o utilities podr√≠an implementar esta soluci√≥n para predecir su rentabilidad en funci√≥n de factores operacionales y contextuales.
# - Puede adaptarse a distintos sectores, siempre que se disponga de registros hist√≥ricos de costos e ingresos.
# - Su integraci√≥n con **Streamlit** y **Power BI** permite la adopci√≥n r√°pida por parte de gerentes financieros, analistas y equipos operativos.
# 


